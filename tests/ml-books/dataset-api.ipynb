{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Using TensorFlow backend.\n",
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gopala KR \n",
      "last updated: 2018-02-28 \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.2.1\n",
      "\n",
      "watermark 1.6.0\n",
      "numpy 1.14.1\n",
      "matplotlib 2.1.2\n",
      "nltk 3.2.5\n",
      "sklearn 0.19.1\n",
      "tensorflow 1.5.0\n",
      "theano 1.0.1\n",
      "mxnet 1.1.0\n",
      "chainer 3.4.0\n",
      "seaborn 0.8.1\n",
      "keras 2.1.4\n",
      "tflearn n\u0007\n",
      "bokeh 0.12.14\n",
      "gensim 3.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "#load watermark\n",
    "%load_ext watermark\n",
    "%watermark -a 'Gopala KR' -u -d -v -p watermark,numpy,matplotlib,nltk,sklearn,tensorflow,theano,mxnet,chainer,seaborn,keras,tflearn,bokeh,gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow's Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides users with multiple options for providing data to the model. One of the probably most common methods is to define placeholders in the TensorFlow graph and feed the data from the current Python session into the TensorFlow `Session` using the `feed_dict` parameter. Using this approach, a large dataset that does not fit into memory is most conveniently and efficiently stored using NumPy archives as explained in [Chunking an Image Dataset for Minibatch Training using NumPy NPZ Archives](image-data-chunking-npz.ipynb) or HDF5 data base files ([Storing an Image Dataset for Minibatch Training using HDF5](image-data-chunking-hdf5.ipynb)).\n",
    "\n",
    "Another approach, which is often preferred when it comes to computational efficiency, is to do the \"data loading\" directly in the graph using input queues from so-called TFRecords files, which is illustrated in the [Using Input Pipelines to Read Data from TFRecords Files](tfrecords.ipynb) notebook. \n",
    "\n",
    "Now, one could also use inpute input queues to load the data directly on the graph [Using Queue Runners to Feed Images Directly from Disk](file-queues.ipynb). The examples in this Jupyter notebook present an alternative to this manual approach, using TensorFlow's \"new\" Dataset API, which is described in more detail here: https://www.tensorflow.org/programmers_guide/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend we have a directory of images containing two subdirectories with images for training, validation, and testing. The following function will create such a dataset of images in JPEG format locally for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Note that executing the following code \n",
    "# cell will download the MNIST dataset\n",
    "# and save all the 60,000 images as separate JPEG\n",
    "# files. This might take a few minutes depending\n",
    "# on your machine.\n",
    "\n",
    "import numpy as np\n",
    "from helper import mnist_export_to_jpg\n",
    "\n",
    "np.random.seed(123)\n",
    "mnist_export_to_jpg(path='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mnist_export_to_jpg` function called above creates 3 directories, mnist_train, mnist_test, and mnist_validation. Note that the names of the subdirectories correspond directly to the class label of the images that are stored under it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_train subdirectories ['6', '5', '2', '1', '7', '8', '4', '9', '3', '0']\n",
      "mnist_valid subdirectories ['6', '5', '2', '1', '7', '8', '4', '9', '3', '0']\n",
      "mnist_test subdirectories ['6', '5', '2', '1', '7', '8', '4', '9', '3', '0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i in ('train', 'valid', 'test'): \n",
    "    dirs = [d for d in os.listdir('mnist_%s' % i) if not d.startswith('.')]\n",
    "    print('mnist_%s subdirectories' % i, dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the images look okay, the snippet below plots an example image from the subdirectory `mnist_train/9/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c109f76a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "some_img = os.path.join('./mnist_train/9/', os.listdir('./mnist_train/9/')[0])\n",
    "\n",
    "img = mpimg.imread(some_img)\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap='binary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The JPEG format introduces a few artifacts that we can see in the image above. In this case, we use JPEG instead of PNG. Here, JPEG is used for demonstration purposes since that's still format many image datasets are stored in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's new Dataset API introduces several new concepts, with the goal of feeding data to the computational graph more efficiently (compared to other approaches like the session's `feed_dicts` that insert values into the \"placeholders\" of a graph). In this context, a `Dataset` can be understood as a collection of \"elements,\" where each element is composed of multiple tensors. In turn, each element might contain several objects of different types. \n",
    "\n",
    "But before we jump further into creating \"datasets,\" let's generate Python lists of the image paths and image labels from the MNIST dataset that we prepared in the first section. \n",
    "\n",
    "Note that TensorFlow datasets can be shuffled, and we will use the `shuffle` operation later on, however, the shuffling operation only shuffles a subset of the whole dataset. The number of elements being shuffled is determined by the `buffer_size` parameter, which also determines how many elements are loaded into memory for shuffling. And since we typically cannot load all elements into memory, it is a good idea to shuffle our dataset, which might be organized by class labels, upfront.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "train_paths = glob.glob('mnist_train/**/*.jpg', recursive=True)\n",
    "train_labels = [int(s.split('/')[1]) for s in train_paths]\n",
    "tmp = list(zip(train_paths, train_labels))\n",
    "random.shuffle(tmp)\n",
    "train_paths, train_labels = zip(*tmp)\n",
    "\n",
    "valid_paths = glob.glob('mnist_valid/**/*.jpg', recursive=True)\n",
    "valid_labels = [int(s.split('/')[1]) for s in valid_paths]\n",
    "tmp = list(zip(valid_paths, valid_labels))\n",
    "random.shuffle(tmp)\n",
    "valid_paths, valid_labels = zip(*tmp)\n",
    "\n",
    "test_paths = glob.glob('mnist_test/**/*.jpg', recursive=True)\n",
    "test_labels = [int(s.split('/')[1]) for s in test_paths]\n",
    "tmp = list(zip(test_paths, test_labels))\n",
    "random.shuffle(tmp)\n",
    "test_paths, test_labels = zip(*tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating Python lists storing the image paths and corresponding class labels, we can convert these to TensorFlow tensors and use the `Dataset`'s `from_tensor_slice` function to create dataset objects. Based on those `Dataset` objects, we can then create an `Iterator` object to fetch data examples form the \"dataset;\" the \"fetching\" operation is called `.get_next()`. Finally, we need initializers for the different datasets (train, validation, and test), if we want to use them in a TensorFlow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "data_g1 = tf.Graph()\n",
    "\n",
    "with data_g1.as_default():\n",
    "    \n",
    "    # setup tensor elements for the dataset\n",
    "    tf_train_paths = tf.constant(train_paths)\n",
    "    tf_train_labels = tf.constant(train_labels)\n",
    "    tf_valid_paths = tf.constant(valid_paths)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    tf_test_paths = tf.constant(test_paths)\n",
    "    tf_test_labels = tf.constant(test_labels)\n",
    "    \n",
    "    \n",
    "    # construct datasets from tf.Tensor objects\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((tf_train_paths,\n",
    "                                                        tf_train_labels)) \n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices((tf_valid_paths,\n",
    "                                                        tf_valid_labels)) \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((tf_test_paths,\n",
    "                                                       tf_test_labels)) \n",
    "    \n",
    "    # initializing iterator to extract elements from the dataset\n",
    "    #   Note: only need 1 iterator, since validation and test \n",
    "    #   datasets have the same image shapes\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "    \n",
    "    # define op that fetches the next element from the iterator\n",
    "    next_element = iterator.get_next()\n",
    "    \n",
    "    # define initializers for the iterator\n",
    "    train_iter_init = iterator.make_initializer(train_dataset)\n",
    "    valid_iter_init = iterator.make_initializer(valid_dataset)\n",
    "    test_iter_init = iterator.make_initializer(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after this quite elaborate setup, let us fetch data examples in a TensorFlow session to make sure that our setup works as intended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch element #1 from training dataset:\n",
      "(b'mnist_train/8/31749.jpg', 8)\n",
      "Fetch element #2 from training dataset:\n",
      "(b'mnist_train/1/25870.jpg', 1)\n",
      "Fetch element #3 from training dataset:\n",
      "(b'mnist_train/3/02099.jpg', 3)\n",
      "\n",
      "Fetch element #1 from validation dataset:\n",
      "(b'mnist_valid/6/45376.jpg', 6)\n",
      "Fetch element #2 from validation dataset:\n",
      "(b'mnist_valid/4/49419.jpg', 4)\n",
      "Fetch element #3 from validation dataset:\n",
      "(b'mnist_valid/9/45902.jpg', 9)\n",
      "\n",
      "Fetch element #1 from test dataset:\n",
      "(b'mnist_test/2/09055.jpg', 2)\n",
      "Fetch element #2 from test dataset:\n",
      "(b'mnist_test/0/01447.jpg', 0)\n",
      "Fetch element #3 from test dataset:\n",
      "(b'mnist_test/8/02945.jpg', 8)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=data_g1) as sess:\n",
    "\n",
    "    sess.run(train_iter_init)\n",
    "    for i in range(3):\n",
    "        print('Fetch element #%d from training dataset:' % (i+1))\n",
    "        ele = sess.run(next_element)\n",
    "        print(ele)\n",
    "    \n",
    "    print()\n",
    "    sess.run(valid_iter_init)\n",
    "    for i in range(3):\n",
    "        print('Fetch element #%d from validation dataset:' % (i+1))\n",
    "        ele = sess.run(next_element)\n",
    "        print(ele)\n",
    "        \n",
    "    print()\n",
    "    sess.run(test_iter_init)\n",
    "    for i in range(3):\n",
    "        print('Fetch element #%d from test dataset:' % (i+1))\n",
    "        ele = sess.run(next_element)\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the procedure we defined only yields the file paths and class lables that we previously stored in Python lists. However, this is a good sanity check before we jump to the next step and read and prepare image files for machine learning or deep learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` API allows us to provide custom preprocessing functions that we can apply to our images. Since we are working with a relatively simple dataset such as MNIST, our preprocessing simply consists of reading in the image from a JPEG file, casting it to the correct type, normalizing the pixels to [0, 1] range, and perform a one-hot encoding on the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_jpg_onehot(path, label):\n",
    "    str_tensor = tf.read_file(path)\n",
    "    decoded_image = tf.image.decode_jpeg(str_tensor,\n",
    "                                         channels=1,\n",
    "                                         fancy_upscaling=False)\n",
    "    # normalize to [0, 1] range\n",
    "    decoded_image = tf.cast(decoded_image, tf.float32)\n",
    "    decoded_image = decoded_image / 255.\n",
    "    # depth=10 because we have 10 mnist class labels\n",
    "    onehot_label = tf.one_hot(label, depth=10)\n",
    "    return decoded_image, onehot_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using the `map` function (shown in the next code snippet), we will also shuffle the dataset and yield batches (instead of single training examples). For simplicity, we omit the test dataset, but its preparation is analogous to the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "def datareader():\n",
    "    tf_train_paths = tf.constant(train_paths)\n",
    "    tf_train_labels = tf.constant(train_labels)\n",
    "    tf_valid_paths = tf.constant(valid_paths)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((tf_train_paths,\n",
    "                                                        tf_train_labels)) \n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices((tf_valid_paths,\n",
    "                                                        tf_valid_labels)) \n",
    "    \n",
    "    ############################################################\n",
    "    ## Custom data transformation; \n",
    "    #  here: image reading, shuffling, batching\n",
    "    train_dataset = train_dataset.map(read_image_jpg_onehot,\n",
    "                                      num_parallel_calls=4)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1000)\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    valid_dataset = valid_dataset.map(read_image_jpg_onehot,\n",
    "                                      num_parallel_calls=4)\n",
    "    valid_dataset = valid_dataset.batch(BATCH_SIZE)\n",
    "    ############################################################\n",
    "\n",
    "    iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "\n",
    "    next_element = iterator.get_next(name='next_element')\n",
    "    \n",
    "    train_iter_init = iterator.make_initializer(train_dataset,\n",
    "                                                name='train_iter_init')\n",
    "    valid_iter_init = iterator.make_initializer(valid_dataset,\n",
    "                                                name='valid_iter_init')\n",
    "    \n",
    "    return next_element\n",
    "\n",
    "\n",
    "data_g2 = tf.Graph()\n",
    "with data_g2.as_default():\n",
    "    datareader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch batch #1 from training dataset:\n",
      "(128, 28, 28, 1) (128, 10)\n",
      "Fetch batch #2 from training dataset:\n",
      "(128, 28, 28, 1) (128, 10)\n",
      "Fetch batch #3 from training dataset:\n",
      "(128, 28, 28, 1) (128, 10)\n",
      "\n",
      "Fetch batch #1 from validation dataset:\n",
      "(128, 28, 28, 1) (128, 10)\n",
      "Fetch batch #2 from validation dataset:\n",
      "(128, 28, 28, 1) (128, 10)\n",
      "Fetch batch #3 from validation dataset:\n",
      "(128, 28, 28, 1) (128, 10)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=data_g2) as sess:\n",
    "\n",
    "    sess.run('train_iter_init')\n",
    "    for i in range(3):\n",
    "        print('Fetch batch #%d from training dataset:' % (i+1))\n",
    "        images, labels = sess.run(['next_element:0', 'next_element:1'])\n",
    "        print(images.shape, labels.shape)\n",
    "        \n",
    "    print()\n",
    "    sess.run('valid_iter_init')\n",
    "    for i in range(3):\n",
    "        print('Fetch batch #%d from validation dataset:' % (i+1))\n",
    "        images, labels = sess.run(['next_element:0', 'next_element:1'])\n",
    "        print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the Dataset API to train a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code example illustrates how we can use the `Dataset` API to train a simple 2-layer multilayer perceptron. Note that we previously defined as `datareader()` function so that we can insert here instead of copy & pasting the code into the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py:539: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "n_epochs = 15\n",
    "n_iter = n_epochs * (len(train_paths) // BATCH_SIZE)\n",
    "\n",
    "# Architecture\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 256\n",
    "height, width = 28, 28\n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "### GRAPH DEFINITION\n",
    "##########################\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    tf.set_random_seed(123)\n",
    "\n",
    "    # Input data\n",
    "    next_element = datareader()\n",
    "    \n",
    "    tf_images = tf.placeholder_with_default(next_element[0],\n",
    "                                            shape=[None, 28, 28, 1], \n",
    "                                            name='images')\n",
    "    tf_labels = tf.placeholder_with_default(next_element[1], \n",
    "                                            shape=[None, 10], \n",
    "                                            name='labels')\n",
    "    \n",
    "    tf_images = tf.reshape(tf_images, (tf.shape(tf_images)[0], 784))\n",
    "    tf_images = tf.cast(tf_images, dtype=tf.float32)\n",
    "\n",
    "    # Model parameters\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([height*width, n_hidden_1], stddev=0.1)),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev=0.1)),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes], stddev=0.1))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Multilayer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(tf_images, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=out_layer, labels=tf_labels)\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train = optimizer.minimize(cost, name='train')\n",
    "\n",
    "    # Prediction\n",
    "    prediction = tf.argmax(out_layer, 1, name='prediction')\n",
    "    correct_prediction = tf.equal(tf.argmax(tf_labels, 1), tf.argmax(out_layer, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after defining the computational graph, the next code snippet will train the multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | AvgCost: 0.007\n",
      "Epoch: 002 | AvgCost: 0.475\n",
      "Epoch: 003 | AvgCost: 0.235\n",
      "Epoch: 004 | AvgCost: 0.182\n",
      "Epoch: 005 | AvgCost: 0.150\n",
      "Epoch: 006 | AvgCost: 0.127\n",
      "Epoch: 007 | AvgCost: 0.111\n",
      "Epoch: 008 | AvgCost: 0.097\n",
      "Epoch: 009 | AvgCost: 0.086\n",
      "Epoch: 010 | AvgCost: 0.077\n",
      "Epoch: 011 | AvgCost: 0.069\n",
      "Epoch: 012 | AvgCost: 0.062\n",
      "Epoch: 013 | AvgCost: 0.055\n",
      "Epoch: 014 | AvgCost: 0.050\n",
      "Epoch: 015 | AvgCost: 0.045\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run('train_iter_init')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver0 = tf.train.Saver()\n",
    "    \n",
    "    avg_cost = 0.\n",
    "    iter_per_epoch = n_iter // n_epochs\n",
    "    epoch = 0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        _, cost = sess.run(['train', 'cost:0'])\n",
    "        avg_cost += cost\n",
    "        \n",
    "        if not i % iter_per_epoch:\n",
    "            epoch += 1\n",
    "            avg_cost /= iter_per_epoch\n",
    "            print(\"Epoch: %03d | AvgCost: %.3f\" % (epoch, avg_cost))\n",
    "            avg_cost = 0.\n",
    "            sess.run('train_iter_init')\n",
    "    \n",
    "    saver0.save(sess, save_path='./mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how we can feed new data points to the network that are not part of the training queue, let's use the test dataset and load the images into Python and pass it to the graph using a `feed_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | AvgCost: 0.007\n",
      "Epoch: 002 | AvgCost: 0.475\n",
      "Epoch: 003 | AvgCost: 0.235\n",
      "Epoch: 004 | AvgCost: 0.182\n",
      "Epoch: 005 | AvgCost: 0.150\n",
      "Epoch: 006 | AvgCost: 0.127\n",
      "Epoch: 007 | AvgCost: 0.111\n",
      "Epoch: 008 | AvgCost: 0.097\n",
      "Epoch: 009 | AvgCost: 0.086\n",
      "Epoch: 010 | AvgCost: 0.077\n",
      "Epoch: 011 | AvgCost: 0.069\n",
      "Epoch: 012 | AvgCost: 0.062\n",
      "Epoch: 013 | AvgCost: 0.055\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run('train_iter_init')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver0 = tf.train.Saver()\n",
    "    \n",
    "    avg_cost = 0.\n",
    "    iter_per_epoch = n_iter // n_epochs\n",
    "    epoch = 0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        _, cost = sess.run(['train', 'cost:0'])\n",
    "        avg_cost += cost\n",
    "        \n",
    "        if not i % iter_per_epoch:\n",
    "            epoch += 1\n",
    "            avg_cost /= iter_per_epoch\n",
    "            print(\"Epoch: %03d | AvgCost: %.3f\" % (epoch, avg_cost))\n",
    "            avg_cost = 0.\n",
    "            sess.run('train_iter_init')\n",
    "    \n",
    "    saver0.save(sess, save_path='./mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feeding new datapoints through placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how we can feed new data points to the network that are not part of the training queue, let's use the test dataset and load the images into Python and pass it to the graph using a `feed_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "img_paths = np.array([p for p in glob.iglob('mnist_test/*/*.jpg')])\n",
    "labels = np.array([int(path.split('/')[1]) for path in img_paths])\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver1 = tf.train.import_meta_graph('./mlp.meta')\n",
    "    saver1.restore(sess, save_path='./mlp')\n",
    "    \n",
    "    num_correct = 0\n",
    "    cnt = 0\n",
    "    for path, lab in zip(img_paths, labels):\n",
    "        cnt += 1\n",
    "        image = mpimg.imread(path)\n",
    "        image = image.reshape(1, 28, 28, 1)\n",
    "        \n",
    "        pred = sess.run('prediction:0', \n",
    "                         feed_dict={'images:0': image})\n",
    "\n",
    "        num_correct += int(lab == pred[0])\n",
    "    acc = num_correct / cnt * 100\n",
    "\n",
    "print('Test accuracy: %.1f%%' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
