{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Using TensorFlow backend.\n",
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gopala KR \n",
      "last updated: 2018-02-28 \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.2.1\n",
      "\n",
      "watermark 1.6.0\n",
      "numpy 1.14.1\n",
      "matplotlib 2.1.2\n",
      "nltk 3.2.5\n",
      "sklearn 0.19.1\n",
      "tensorflow 1.5.0\n",
      "theano 1.0.1\n",
      "mxnet 1.1.0\n",
      "chainer 3.4.0\n",
      "seaborn 0.8.1\n",
      "keras 2.1.4\n",
      "tflearn n\u0007\n",
      "bokeh 0.12.14\n",
      "gensim 3.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "#load watermark\n",
    "%load_ext watermark\n",
    "%watermark -a 'Gopala KR' -u -d -v -p watermark,numpy,matplotlib,nltk,sklearn,tensorflow,theano,mxnet,chainer,seaborn,keras,tflearn,bokeh,gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Input Pipelines to Read Data from TFRecords Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides users with multiple options for providing data to the model. One of the probably most common methods is to define placeholders in the TensorFlow graph and feed the data from the current Python session into the TensorFlow `Session` using the `feed_dict` parameter. Using this approach, a large dataset that does not fit into memory is most conveniently and efficiently stored using NumPy archives as explained in [Chunking an Image Dataset for Minibatch Training using NumPy NPZ Archives](image-data-chunking-npz.ipynb) or HDF5 data base files ([Storing an Image Dataset for Minibatch Training using HDF5](image-data-chunking-hdf5.ipynb)).\n",
    "\n",
    "Another approach, which is often preferred when it comes to computational efficiency, is to do the \"data loading\" directly in the graph using input queues from so-called TFRecords files, which will be illustrated in this notebook.\n",
    "\n",
    "Beyond the examples in this notebook, you are encouraged to read more in TensorFlow's \"[Reading Data](https://www.tensorflow.org/programmers_guide/reading_data)\" guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend we have a directory of images containing two subdirectories with images for training, validation, and testing. The following function will create such a dataset of images in JPEG format locally for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Note that executing the following code \n",
    "# cell will download the MNIST dataset\n",
    "# and save all the 60,000 images as separate JPEG\n",
    "# files. This might take a few minutes depending\n",
    "# on your machine.\n",
    "\n",
    "import numpy as np\n",
    "from helper import mnist_export_to_jpg\n",
    "\n",
    "np.random.seed(123)\n",
    "mnist_export_to_jpg(path='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mnist_export_to_jpg` function called above creates 3 directories, mnist_train, mnist_test, and mnist_validation. Note that the names of the subdirectories correspond directly to the class label of the images that are stored under it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_train subdirectories ['6', '5', '2', '1', '7', '8', '4', '9', '3', '0']\n",
      "mnist_valid subdirectories ['6', '5', '2', '1', '7', '8', '4', '9', '3', '0']\n",
      "mnist_test subdirectories ['6', '5', '2', '1', '7', '8', '4', '9', '3', '0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i in ('train', 'valid', 'test'): \n",
    "    dirs = [d for d in os.listdir('mnist_%s' % i) if not d.startswith('.')]\n",
    "    print('mnist_%s subdirectories' % i, dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the images look okay, the snippet below plots an example image from the subdirectory `mnist_train/9/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f442242e6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "some_img = os.path.join('./mnist_train/9/', os.listdir('./mnist_train/9/')[0])\n",
    "\n",
    "img = mpimg.imread(some_img)\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap='binary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The JPEG format introduces a few artifacts that we can see in the image above. In this case, we use JPEG instead of PNG. Here, JPEG is used for demonstration purposes since that's still format many image datasets are stored in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Saving images as TFRecords files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to convert the images into a binary TFRecords file, which is based on Google's [protocol buffer](https://developers.google.com/protocol-buffers/) format:\n",
    "\n",
    "> The recommended format for TensorFlow is a TFRecords file containing tf.train.Example protocol buffers (which contain Features as a field). You write a little program that gets your data, stuffs it in an Example protocol buffer, serializes the protocol buffer to a string, and then writes the string to a TFRecords file using the tf.python_io.TFRecordWriter. For example, tensorflow/examples/how_tos/reading_data/convert_to_records.py converts MNIST data to this format.  \n",
    "\n",
    "> [ Excerpt from https://www.tensorflow.org/programmers_guide/reading_data ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def images_to_tfrecords(data_stempath='./mnist_',\n",
    "                        shuffle=False, \n",
    "                        random_seed=None):\n",
    "    \n",
    "    def int64_to_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "    \n",
    "    for s in ['train', 'valid', 'test']:\n",
    "\n",
    "        with tf.python_io.TFRecordWriter('mnist_%s.tfrecords' % s) as writer:\n",
    "\n",
    "            img_paths = np.array([p for p in glob.iglob('%s%s/**/*.jpg' % \n",
    "                                  (data_stempath, s), \n",
    "                                   recursive=True)])\n",
    "\n",
    "            if shuffle:\n",
    "                rng = np.random.RandomState(random_seed)\n",
    "                rng.shuffle(img_paths)\n",
    "\n",
    "            for idx, path in enumerate(img_paths):\n",
    "                label = int(os.path.basename(os.path.dirname(path)))\n",
    "                image = mpimg.imread(path)\n",
    "                image = image.reshape(-1).tolist()\n",
    "\n",
    "\n",
    "                example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'image': int64_to_feature(image),\n",
    "                    'label': int64_to_feature([label])}))\n",
    "\n",
    "                writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is important to shuffle the dataset so that we can later make use of TensorFlow's [`tf.train.shuffle_batch`](https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch) function and don't need to load the whole dataset into memory to shuffle epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_tfrecords(shuffle=True, random_seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure that the images were serialized correctly, let us load an image back from TFRecords using the [`tf.python_io.tf_record_iterator`](https://www.tensorflow.org/api_docs/python/tf/python_io/tf_record_iterator) and display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f442235fbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "record_iterator = tf.python_io.tf_record_iterator(path='mnist_train.tfrecords')\n",
    "\n",
    "for r in record_iterator:\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(r)\n",
    "    \n",
    "    label = example.features.feature['label'].int64_list.value[0]\n",
    "    print('Label:', label)\n",
    "    img = np.array(example.features.feature['image'].int64_list.value)\n",
    "    img = img.reshape((28, 28))\n",
    "    plt.imshow(img, cmap='binary')\n",
    "    plt.show\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, the image above looks okay. In the next secction, we will introduce a slightly different approach for loading the images, namely, the [`TFRecordReader`](https://www.tensorflow.org/api_docs/python/tf/TFRecordReader), which we need to load images inside a TensorFlow graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading images via the TFRecordReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly speaking, we can regard the [`TFRecordReader`](https://www.tensorflow.org/api_docs/python/tf/TFRecordReader) as a class that let's us load images \"symbolically\" inside a TensorFlow graph. A `TFRecordReader`  uses the state in the graph to remember the location of a `.tfrecord` file that it reads and lets us iterate over training examples and batches after initializing the graph as we will see later.\n",
    "\n",
    "To see how it works, let's start with a simple function that reads one image at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_image(tfrecords_queue, normalize=True):\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    key, value = reader.read(tfrecords_queue)\n",
    "    features = tf.parse_single_example(value,\n",
    "        features={'label': tf.FixedLenFeature([], tf.int64),\n",
    "                  'image': tf.FixedLenFeature([784], tf.int64)})\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    image = tf.cast(features['image'], tf.float32)\n",
    "    onehot_label = tf.one_hot(indices=label, depth=10)\n",
    "    \n",
    "    if normalize:\n",
    "        # normalize to [0, 1] range\n",
    "        image = image / 255.\n",
    "    \n",
    "    return onehot_label, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this `read_one_image` function to fetch images in a TensorFlow session, we will make use of queue runners as illustrated in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] \n",
      "Image dimensions: (784,)\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    queue = tf.train.string_input_producer(['mnist_train.tfrecords'], \n",
    "                                           num_epochs=10)\n",
    "    label, image = read_one_image(queue)\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "   \n",
    "    for i in range(10):\n",
    "        one_label, one_image = sess.run([label, image])\n",
    "        \n",
    "    print('Label:', one_label, '\\nImage dimensions:', one_image.shape)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `tf.train.string_input_producer` produces a filename queue that we iterate over in the session. Note that we need to call `sess.run(tf.local_variables_initializer())` if we define a fixed number of `num_epochs` in `tf.train.string_input_producer`. Alternatively, `num_epochs` can be set to `None` to iterate \"infinitely.\" \n",
    "\n",
    "- The `tf.train.start_queue_runners` function uses a queue runner that uses a separate thread to load the filenames from the `queue` that we defined in the graph without blocking the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we rarely (want to) train neural networks with one datapoint at a time but use minibatches instead. TensorFlow also has some really convenient utility functions to do the batching conveniently. In the following code example, we will use the [`tf.train.shuffle_batch`](https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch) function to load the images and labels in batches of size 64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    queue = tf.train.string_input_producer(['mnist_train.tfrecords'], \n",
    "                                           num_epochs=10)\n",
    "    label, image = read_one_image(queue)\n",
    "    \n",
    "    \n",
    "    label_batch, image_batch = tf.train.shuffle_batch([label, image], \n",
    "                                                       batch_size=64,\n",
    "                                                       capacity=5000,\n",
    "                                                       min_after_dequeue=2000,\n",
    "                                                       num_threads=8,\n",
    "                                                       seed=123)\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "   \n",
    "    for i in range(10):\n",
    "        many_labels, many_images = sess.run([label_batch, image_batch])\n",
    "        \n",
    "    print('Batch size:', many_labels.shape[0])\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other relevant arguments we provided to `tf.train.shuffle_batch` are described below:\n",
    "\n",
    "- `capacity`: An integer that defines the maximum number of elements in the queue.\n",
    "- `min_after_dequeue`: The minimum number elements in the queue after a dequeue, which is used to ensure that a minimum number of data points have been loaded for shuffling.\n",
    "- `num_threads`: The number of threads for enqueuing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use queue runners to train a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will take the concepts that were introduced in the previous sections and train a multilayer perceptron from the `'mnist_train.tfrecords'` file:\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py:539: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-d73622515213>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | AvgCost: 0.007\n",
      "Epoch: 002 | AvgCost: 0.469\n",
      "Epoch: 003 | AvgCost: 0.239\n",
      "Epoch: 004 | AvgCost: 0.184\n",
      "Epoch: 005 | AvgCost: 0.150\n",
      "Epoch: 006 | AvgCost: 0.127\n",
      "Epoch: 007 | AvgCost: 0.110\n",
      "Epoch: 008 | AvgCost: 0.098\n",
      "Epoch: 009 | AvgCost: 0.087\n",
      "Epoch: 010 | AvgCost: 0.078\n",
      "Epoch: 011 | AvgCost: 0.069\n",
      "Epoch: 012 | AvgCost: 0.064\n",
      "Epoch: 013 | AvgCost: 0.057\n",
      "Epoch: 014 | AvgCost: 0.052\n",
      "Epoch: 015 | AvgCost: 0.047\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "n_epochs = 15\n",
    "n_iter = n_epochs * (45000 // batch_size)\n",
    "\n",
    "# Architecture\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 256\n",
    "height, width = 28, 28\n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "### GRAPH DEFINITION\n",
    "##########################\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    tf.set_random_seed(123)\n",
    "\n",
    "    # Input data\n",
    "    queue = tf.train.string_input_producer(['mnist_train.tfrecords'], \n",
    "                                           num_epochs=None)\n",
    "    label, image = read_one_image(queue)\n",
    "    \n",
    "    label_batch, image_batch = tf.train.shuffle_batch([label, image], \n",
    "                                                       batch_size=batch_size,\n",
    "                                                       seed=123,\n",
    "                                                       num_threads=8,\n",
    "                                                       capacity=5000,\n",
    "                                                       min_after_dequeue=2000)\n",
    "    \n",
    "    tf_images = tf.placeholder_with_default(image_batch,\n",
    "                                            shape=[None, 784], \n",
    "                                            name='images')\n",
    "    tf_labels = tf.placeholder_with_default(label_batch, \n",
    "                                            shape=[None, 10], \n",
    "                                            name='labels')\n",
    "\n",
    "    # Model parameters\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([height*width, n_hidden_1], stddev=0.1)),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev=0.1)),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes], stddev=0.1))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Multilayer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(tf_images, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=tf_labels)\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train = optimizer.minimize(cost, name='train')\n",
    "\n",
    "    # Prediction\n",
    "    prediction = tf.argmax(out_layer, 1, name='prediction')\n",
    "    correct_prediction = tf.equal(tf.argmax(label_batch, 1), tf.argmax(out_layer, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver0 = tf.train.Saver()\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    avg_cost = 0.\n",
    "    iter_per_epoch = n_iter // n_epochs\n",
    "    epoch = 0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        _, cost = sess.run(['train', 'cost:0'])\n",
    "        avg_cost += cost\n",
    "        \n",
    "        if not i % iter_per_epoch:\n",
    "            epoch += 1\n",
    "            avg_cost /= iter_per_epoch\n",
    "            print(\"Epoch: %03d | AvgCost: %.3f\" % (epoch, avg_cost))\n",
    "            avg_cost = 0.\n",
    "            \n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "    saver0.save(sess, save_path='./mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the graph above, you probably wondered why we used [`tf.placeholder_with_default`](https://www.tensorflow.org/api_docs/python/tf/placeholder_with_default) to define the two placeholders:\n",
    "\n",
    "```python\n",
    "tf_images = tf.placeholder_with_default(image_batch,\n",
    "                                            shape=[None, 784], \n",
    "                                            name='images')\n",
    "tf_labels = tf.placeholder_with_default(label_batch, \n",
    "                                        shape=[None, 10], \n",
    "                                        name='labels')\n",
    "```      \n",
    "\n",
    "In the training session above, these placeholders are being ignored if we don't feed them via a session's `feed_dict`, or in other words \"[A `tf.placeholder_with_default` is a] placeholder op that passes through input when its output is not fed\" (https://www.tensorflow.org/api_docs/python/tf/placeholder_with_default).\n",
    "\n",
    "However, these placeholders are useful if we want to feed new data to the graph and make predictions after training as in a real-world application, which we will see in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feeding new datapoints through placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how we can feed new data points to the network that are not part of the `mnist_train.tfrecords` file, let's use the test dataset and load the images into Python and pass it to the graph using a `feed_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mlp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py:539: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.2%\n"
     ]
    }
   ],
   "source": [
    "record_iterator = tf.python_io.tf_record_iterator(path='mnist_test.tfrecords')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver1 = tf.train.import_meta_graph('./mlp.meta')\n",
    "    saver1.restore(sess, save_path='./mlp')\n",
    "    \n",
    "    num_correct = 0\n",
    "    for idx, r in enumerate(record_iterator):\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(r)\n",
    "        label = example.features.feature['label'].int64_list.value[0]\n",
    "        image = np.array(example.features.feature['image'].int64_list.value)\n",
    "        \n",
    "        pred = sess.run('prediction:0', \n",
    "                         feed_dict={'images:0': image.reshape(1, 784)})\n",
    "\n",
    "        num_correct += int(label == pred[0])\n",
    "    acc = num_correct / (idx + 1) * 100\n",
    "\n",
    "print('Test accuracy: %.1f%%' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
