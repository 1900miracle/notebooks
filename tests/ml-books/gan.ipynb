{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Using TensorFlow backend.\n",
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gopala KR \n",
      "last updated: 2018-02-28 \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.2.1\n",
      "\n",
      "watermark 1.6.0\n",
      "numpy 1.14.1\n",
      "matplotlib 2.1.2\n",
      "nltk 3.2.5\n",
      "sklearn 0.19.1\n",
      "tensorflow 1.5.0\n",
      "theano 1.0.1\n",
      "mxnet 1.1.0\n",
      "chainer 3.4.0\n",
      "seaborn 0.8.1\n",
      "keras 2.1.4\n",
      "tflearn n\u0007\n",
      "bokeh 0.12.14\n",
      "gensim 3.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "#load watermark\n",
    "%load_ext watermark\n",
    "%watermark -a 'Gopala KR' -u -d -v -p watermark,numpy,matplotlib,nltk,sklearn,tensorflow,theano,mxnet,chainer,seaborn,keras,tflearn,bokeh,gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo -- General Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of General Adversarial Nets (GAN) where both the discriminator and generator are multi-layer perceptrons with one hidden layer only. In this example, the GAN generator was trained to generate MNIST images.\n",
    "\n",
    "Uses\n",
    "\n",
    "- samples from a random normal distribution (range [-1, 1])\n",
    "- dropout\n",
    "- leaky relus\n",
    "- ~~batch normalization~~ [performs worse here]\n",
    "- separate batches for \"fake\" and \"real\" images (where the labels are 1 = real images, 0 = fake images)\n",
    "- MNIST images normalized to [-1, 1] range\n",
    "- generator with tanh output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pickle as pkl\n",
    "\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "### Abbreviatiuons\n",
    "# dis_*: discriminator network\n",
    "# gen_*: generator network\n",
    "\n",
    "########################\n",
    "### Helper functions\n",
    "########################\n",
    "\n",
    "def leaky_relu(x, alpha=0.0001):\n",
    "    return tf.maximum(alpha * x, x)\n",
    "\n",
    "\n",
    "########################\n",
    "### DATASET\n",
    "########################\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data')\n",
    "\n",
    "\n",
    "#########################\n",
    "### SETTINGS\n",
    "#########################\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 64\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Other settings\n",
    "print_interval = 200\n",
    "\n",
    "# Architecture\n",
    "dis_input_size = 784\n",
    "gen_input_size = 100\n",
    "\n",
    "dis_hidden_size = 128\n",
    "gen_hidden_size = 128\n",
    "\n",
    "\n",
    "#########################\n",
    "### GRAPH DEFINITION\n",
    "#########################\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # Placeholders for settings\n",
    "    dropout = tf.placeholder(tf.float32, shape=None, name='dropout')\n",
    "    is_training = tf.placeholder(tf.bool, shape=None, name='is_training')\n",
    "    \n",
    "    # Input data\n",
    "    dis_x = tf.placeholder(tf.float32, shape=[None, dis_input_size], name='discriminator_input') \n",
    "    gen_x = tf.placeholder(tf.float32, [None, gen_input_size], name='generator_input')\n",
    "\n",
    "\n",
    "    ##################\n",
    "    # Generator Model\n",
    "    ##################\n",
    "\n",
    "    with tf.variable_scope('generator'):\n",
    "        # linear -> ~~batch norm~~ -> leaky relu -> dropout -> tanh output\n",
    "        gen_hidden = tf.layers.dense(inputs=gen_x, units=gen_hidden_size,\n",
    "                                      activation=None)\n",
    "        #gen_hidden = tf.layers.batch_normalization(gen_hidden, training=is_training)\n",
    "        gen_hidden = leaky_relu(gen_hidden)\n",
    "        gen_hidden = tf.layers.dropout(gen_hidden, rate=dropout_rate)\n",
    "        gen_logits = tf.layers.dense(inputs=gen_hidden, units=dis_input_size, \n",
    "                                     activation=None)\n",
    "        gen_out = tf.tanh(gen_logits, 'generator_output')\n",
    "\n",
    "\n",
    "    ######################\n",
    "    # Discriminator Model\n",
    "    ######################\n",
    "    \n",
    "    def build_discriminator_graph(input_x, reuse=None):\n",
    "        # linear -> ~~batch norm~~ -> leaky relu -> dropout -> sigmoid output\n",
    "        with tf.variable_scope('discriminator', reuse=reuse):\n",
    "            hidden = tf.layers.dense(inputs=input_x, units=dis_hidden_size, \n",
    "                                     activation=None)\n",
    "            #hidden = tf.layers.batch_normalization(hidden, training=is_training)\n",
    "            hidden = leaky_relu(hidden)\n",
    "            hidden = tf.layers.dropout(hidden, rate=dropout_rate)\n",
    "            logits = tf.layers.dense(inputs=hidden, units=1, activation=None)\n",
    "            out = tf.sigmoid(logits)\n",
    "        return logits, out    \n",
    "\n",
    "    # Create a discriminator for real data and a discriminator for fake data\n",
    "    dis_real_logits, dis_real_out = build_discriminator_graph(dis_x, reuse=False)\n",
    "    dis_fake_logits, dis_fake_out = build_discriminator_graph(gen_out, reuse=True)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    # Generator and Discriminator Losses\n",
    "    #####################################\n",
    "    \n",
    "    # Two discriminator cost components: loss on real data + loss on fake data\n",
    "    # Real data has class label 0, fake data has class label 1\n",
    "    dis_real_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_real_logits, \n",
    "                                                            labels=tf.zeros_like(dis_real_logits))\n",
    "    dis_fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_fake_logits, \n",
    "                                                            labels=tf.ones_like(dis_fake_logits))\n",
    "    dis_cost = tf.add(tf.reduce_mean(dis_fake_loss), \n",
    "                      tf.reduce_mean(dis_real_loss), \n",
    "                      name='discriminator_cost')\n",
    " \n",
    "    # Generator cost: difference between dis. prediction and label \"0\" for real images\n",
    "    gen_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=dis_fake_logits,\n",
    "                                                       labels=tf.zeros_like(dis_fake_logits))\n",
    "    gen_cost = tf.reduce_mean(gen_loss, name='generator_cost')\n",
    "    \n",
    "    \n",
    "    #########################################\n",
    "    # Generator and Discriminator Optimizers\n",
    "    #########################################\n",
    "      \n",
    "    dis_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    dis_train_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator')\n",
    "    dis_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='discriminator')\n",
    "    \n",
    "    with tf.control_dependencies(dis_update_ops): # required to upd. batch_norm params\n",
    "        dis_train = dis_optimizer.minimize(dis_cost, var_list=dis_train_vars,\n",
    "                                           name='train_discriminator')\n",
    "    \n",
    "    gen_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gen_train_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')\n",
    "    gen_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='generator')\n",
    "    \n",
    "    with tf.control_dependencies(gen_update_ops): # required to upd. batch_norm params\n",
    "        gen_train = gen_optimizer.minimize(gen_cost, var_list=gen_train_vars,\n",
    "                                           name='train_generator')\n",
    "    \n",
    "    # Saver to save session for reuse\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### TRAINING & EVALUATION\n",
    "##########################\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    avg_costs = {'discriminator': [], 'generator': []}\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        dis_avg_cost, gen_avg_cost = 0., 0.\n",
    "        total_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            batch_x = batch_x*2 - 1 # normalize\n",
    "            batch_randsample = np.random.uniform(-1, 1, size=(batch_size, gen_input_size))\n",
    "            \n",
    "            # Train\n",
    "            _, dc = sess.run(['train_discriminator', 'discriminator_cost:0'],\n",
    "                             feed_dict={'discriminator_input:0': batch_x, \n",
    "                                        'generator_input:0': batch_randsample,\n",
    "                                        'dropout:0': dropout_rate,\n",
    "                                        'is_training:0': True})\n",
    "            _, gc = sess.run(['train_generator', 'generator_cost:0'],\n",
    "                             feed_dict={'generator_input:0': batch_randsample,\n",
    "                                        'dropout:0': dropout_rate,\n",
    "                                        'is_training:0': True})\n",
    "            \n",
    "            dis_avg_cost += dc\n",
    "            gen_avg_cost += gc\n",
    "\n",
    "            if not i % print_interval:\n",
    "                print(\"Minibatch: %03d | Dis/Gen Cost:    %.3f/%.3f\" % (i + 1, dc, gc))\n",
    "                \n",
    "\n",
    "        print(\"Epoch:     %03d | Dis/Gen AvgCost: %.3f/%.3f\" % \n",
    "              (epoch + 1, dis_avg_cost / total_batch, gen_avg_cost / total_batch))\n",
    "        \n",
    "        avg_costs['discriminator'].append(dis_avg_cost / total_batch)\n",
    "        avg_costs['generator'].append(gen_avg_cost / total_batch)\n",
    "    \n",
    "    \n",
    "    saver.save(sess, save_path='./gan.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(avg_costs['discriminator'])), \n",
    "         avg_costs['discriminator'], label='discriminator')\n",
    "plt.plot(range(len(avg_costs['generator'])),\n",
    "         avg_costs['generator'], label='generator')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### RELOAD & GENERATE SAMPLE IMAGES\n",
    "####################################\n",
    "\n",
    "\n",
    "n_examples = 25\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    saver.restore(sess, save_path='./gan.ckpt')\n",
    "\n",
    "    batch_randsample = np.random.uniform(-1, 1, size=(n_examples, gen_input_size))\n",
    "    new_examples = sess.run('generator/generator_output:0',\n",
    "                            feed_dict={'generator_input:0': batch_randsample,\n",
    "                                       'dropout:0': 0.0,\n",
    "                                       'is_training:0': False})\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(8, 8),\n",
    "                         sharey=True, sharex=True)\n",
    "\n",
    "for image, ax in zip(new_examples, axes.flatten()):\n",
    "    ax.imshow(image.reshape((dis_input_size // 28, dis_input_size // 28)), cmap='binary')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
