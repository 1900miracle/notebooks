{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Using TensorFlow backend.\n",
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gopala KR \n",
      "last updated: 2018-02-28 \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.2.1\n",
      "\n",
      "watermark 1.6.0\n",
      "numpy 1.14.1\n",
      "matplotlib 2.1.2\n",
      "nltk 3.2.5\n",
      "sklearn 0.19.1\n",
      "tensorflow 1.5.0\n",
      "theano 1.0.1\n",
      "mxnet 1.1.0\n",
      "chainer 3.4.0\n",
      "seaborn 0.8.1\n",
      "keras 2.1.4\n",
      "tflearn n\u0007\n",
      "bokeh 0.12.14\n",
      "gensim 3.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "#load watermark\n",
    "%load_ext watermark\n",
    "%watermark -a 'Gopala KR' -u -d -v -p watermark,numpy,matplotlib,nltk,sklearn,tensorflow,theano,mxnet,chainer,seaborn,keras,tflearn,bokeh,gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing an Image Dataset for Minibatch Training using HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example for how to save a large dataset of images as Hierarchical Data Format (HDF) for quick access during minibatch learning. This approach uses the common [HDF5](https://support.hdfgroup.org/HDF5/) format and should be accessible to any programming language or tool with an HDF5 API.\n",
    "\n",
    "While this approach performs reasonably well (sufficiently well for my applications), you may also be interested in TensorFlow's \"[Reading Data](https://www.tensorflow.org/programmers_guide/reading_data)\" guide to work with `TfRecords` and file queues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend we have a directory of images containing two subdirectories with images for training, validation, and testing. The following function will create such a dataset of images in JPEG format locally for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Note that executing the following code \n",
    "# cell will download the MNIST dataset\n",
    "# and save all the 60,000 images as separate JPEG\n",
    "# files. This might take a few minutes depending\n",
    "# on your machine.\n",
    "\n",
    "import numpy as np\n",
    "from helper import mnist_export_to_jpg\n",
    "\n",
    "np.random.seed(123)\n",
    "mnist_export_to_jpg(path='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_train subdirectories ['6', '5', '2', '1', '7', '8', '4', '9', '3', '0']\n",
      "mnist_valid subdirectories ['6', '5', '2', '1', '7', '8', '4', '9', '3', '0']\n",
      "mnist_test subdirectories ['6', '5', '2', '1', '7', '8', '4', '9', '3', '0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i in ('train', 'valid', 'test'):\n",
    "    print('mnist_%s subdirectories' % i, os.listdir('mnist_%s' % i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the names of the subdirectories correspond directly to the class label of the images that are stored under it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the images look okay, the snippet below plots an example image from the subdirectory `mnist_train/9/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f023eb266a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "some_img = os.path.join('./mnist_train/9/', os.listdir('./mnist_train/9/')[0])\n",
    "\n",
    "img = mpimg.imread(some_img)\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap='binary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The JPEG format introduces a few artifacts that we can see in the image above. In this case, we use JPEG instead of PNG. Here, JPEG is used for demonstration purposes since that's still format many image datasets are stored in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Saving images as HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following wrapper function creates .h5 file containing training, testing, and validation datasets. It will group images together into larger integer arrays that are then saved as subgroups in the HDF5 file. For instance, the training images will be saved as `'train/images'` and the corresponding labels as `'train/labels'` subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "\n",
    "\n",
    "def images_to_h5(data_stempath='./mnist_',\n",
    "                 width=28, height=28, channels=1,\n",
    "                 shuffle=False, random_seed=None):\n",
    "    \n",
    "    with h5py.File('mnist_batches.h5', 'w') as h5f:\n",
    "    \n",
    "        for s in ['train', 'valid', 'test']:\n",
    "            img_paths = [p for p in glob.iglob('%s%s/**/*.jpg' % \n",
    "                                       (data_stempath, s), \n",
    "                                        recursive=True)]\n",
    "\n",
    "            dset1 = h5f.create_dataset('%s/images' % s, \n",
    "                                       shape=[len(img_paths), \n",
    "                                              width, height, channels], \n",
    "                                       compression=None,\n",
    "                                       dtype='uint8')\n",
    "            dset2 = h5f.create_dataset('%s/labels' % s, \n",
    "                                       shape=[len(img_paths)], \n",
    "                                       compression=None,\n",
    "                                       dtype='uint8')\n",
    "            dset3 = h5f.create_dataset('%s/file_ids' % s, \n",
    "                                       shape=[len(img_paths)], \n",
    "                                       compression=None,\n",
    "                                       dtype='S5')\n",
    "            \n",
    "            rand_indices = np.arange(len(img_paths))\n",
    "            \n",
    "            if shuffle:\n",
    "                rng = np.random.RandomState(random_seed)\n",
    "                rng.shuffle(rand_indices)\n",
    "\n",
    "            for idx, path in enumerate(img_paths):\n",
    "\n",
    "                rand_idx = rand_indices[idx]\n",
    "                label = int(os.path.basename(os.path.dirname(path)))\n",
    "                image = mpimg.imread(path)\n",
    "                dset1[rand_idx] = image.reshape(width, height, channels)\n",
    "                dset2[rand_idx] = label\n",
    "                dset3[rand_idx] = np.array([os.path.basename(path)], dtype='S6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we didn't specify any compression format. The reason is that non-compressed HDF5 datasets are much faster to read, which is an important factor for training deep learning systems. In this case, the dataset is about ~47 Mb in size. However, we are working with larger datasets, compressing the HDF5 dataset might be one easy way to deal with hardware storage limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_h5(shuffle=True, random_seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the archiving worked correctly, we will now load the training images and print the array shape. Note that we can now access each archive similar to a python dictionary. Here the `'data'` key contains the image data and the `'labels'` key stores an array containing the corresponding class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 28, 28, 1)\n",
      "(45000,)\n",
      "(45000,)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('mnist_batches.h5', 'r') as h5f:\n",
    "    print(h5f['train/images'].shape)\n",
    "    print(h5f['train/labels'].shape)\n",
    "    print(h5f['train/file_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 3\n",
      "File ID: b'02427'\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0216efa4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('mnist_batches.h5', 'r') as h5f:\n",
    "\n",
    "    plt.imshow(h5f['train/images'][0][:, :, -1], cmap='binary');\n",
    "    print('Class label:', h5f['train/labels'][0])\n",
    "    print('File ID:', h5f['train/file_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell implements a class for iterating over the MNIST images, based on the .h5 file, conveniently. \n",
    "Via the `normalize` parameter we additionally scale the image pixels to [0, 1] range, which typically helps with gradient-based optimization in practice.\n",
    "\n",
    "The key functions (here: generators) are\n",
    "\n",
    "- load_train_epoch\n",
    "- load_valid_epoch\n",
    "- load_test_epoch\n",
    "\n",
    "These let us iterate over small chunks (determined via `minibatch_size`) and yield minibatches via memory-efficient Python generators. Via the two shuffle parameters, we can further control if the images within each batch to be shuffled. By setting `onehot=True`, the labels are converted into a onehot representation for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader():\n",
    "    def __init__(self, minibatches_path, \n",
    "                 normalize=True):\n",
    "        \n",
    "        self.minibatches_path = minibatches_path\n",
    "        self.normalize = normalize\n",
    "        self.num_train = 45000\n",
    "        self.num_valid = 5000\n",
    "        self.num_test = 10000\n",
    "        self.n_classes = 10\n",
    "\n",
    "\n",
    "    def load_train_epoch(self, batch_size=50, onehot=False,\n",
    "                         shuffle_batch=False, prefetch_batches=1, seed=None):\n",
    "        for batch_x, batch_y in self._load_epoch(which='train',\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 onehot=onehot,\n",
    "                                                 shuffle_batch=shuffle_batch,\n",
    "                                                 prefetch_batches=prefetch_batches, \n",
    "                                                 seed=seed):\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "    def load_test_epoch(self, batch_size=50, onehot=False,\n",
    "                        shuffle_batch=False, prefetch_batches=1, seed=None):\n",
    "        for batch_x, batch_y in self._load_epoch(which='test',\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 onehot=onehot,\n",
    "                                                 shuffle_batch=shuffle_batch,\n",
    "                                                 prefetch_batches=prefetch_batches,\n",
    "                                                 seed=seed):\n",
    "            yield batch_x, batch_y\n",
    "            \n",
    "    def load_validation_epoch(self, batch_size=50, onehot=False,\n",
    "                              shuffle_batch=False, prefetch_batches=1, seed=None):\n",
    "        for batch_x, batch_y in self._load_epoch(which='valid',\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 onehot=onehot,\n",
    "                                                 shuffle_batch=shuffle_batch,\n",
    "                                                 prefetch_batches=prefetch_batches, \n",
    "                                                 seed=seed):\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "    def _load_epoch(self, which='train', batch_size=50, onehot=False,\n",
    "                    shuffle_batch=False, prefetch_batches=1, seed=None):\n",
    "        \n",
    "        prefetch_size = prefetch_batches * batch_size\n",
    "        \n",
    "        if shuffle_batch:\n",
    "            rgen = np.random.RandomState(seed)\n",
    "\n",
    "        with h5py.File(self.minibatches_path, 'r') as h5f:\n",
    "            indices = np.arange(h5f['%s/images' % which].shape[0])\n",
    "            \n",
    "            for start_idx in range(0, indices.shape[0] - prefetch_size + 1,\n",
    "                                   prefetch_size):           \n",
    "            \n",
    "\n",
    "                x_batch = h5f['%s/images' % which][start_idx:start_idx + prefetch_size]\n",
    "                x_batch = x_batch.astype(np.float32)\n",
    "                y_batch = h5f['%s/labels' % which][start_idx:start_idx + prefetch_size]\n",
    "\n",
    "                if onehot:\n",
    "                    y_batch = (np.arange(self.n_classes) == \n",
    "                               y_batch[:, None]).astype(np.uint8)\n",
    "\n",
    "                if self.normalize:\n",
    "                    # normalize to [0, 1] range\n",
    "                    x_batch = x_batch.astype(np.float32) / 255.\n",
    "\n",
    "                if shuffle_batch:\n",
    "                    rand_indices = np.arange(prefetch_size)\n",
    "                    rgen.shuffle(rand_indices)\n",
    "                    x_batch = x_batch[rand_indices]\n",
    "                    y_batch = y_batch[rand_indices]\n",
    "\n",
    "                for batch_idx in range(0, x_batch.shape[0] - batch_size + 1,\n",
    "                                       batch_size):\n",
    "                    \n",
    "                    yield (x_batch[batch_idx:batch_idx + batch_size], \n",
    "                           y_batch[batch_idx:batch_idx + batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following for loop will iterate over the 45,000 training examples in our MNIST training set, yielding 50 images and labels at a time (note that we previously set aside 5000 training example as our validation datast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 28, 28, 1)\n",
      "(50, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_loader = BatchLoader(minibatches_path='./mnist_batches.h5', \n",
    "                           normalize=True)\n",
    "\n",
    "for batch_x, batch_y in batch_loader.load_train_epoch(batch_size=50, onehot=True):\n",
    "    print(batch_x.shape)\n",
    "    print(batch_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One training epoch contains 45000 images\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for batch_x, batch_y in batch_loader.load_train_epoch(\n",
    "        batch_size=100, onehot=True):\n",
    "    cnt += batch_x.shape[0]\n",
    "    \n",
    "print('One training epoch contains %d images' % cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 ms ± 2.28 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def one_epoch():\n",
    "    for batch_x, batch_y in batch_loader.load_train_epoch(\n",
    "            batch_size=100, onehot=True):\n",
    "        pass\n",
    "    \n",
    "% timeit one_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the benchmark above, an iteration over one training epoch (45k images) is relatively fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we could iterate over validation and test data via \n",
    "\n",
    "- batch_loader.load_validation_epoch\n",
    "- batch_loader.load_test_epoch\n",
    "\n",
    "Note that increasing the `batch_size` can substantially improve the computationally efficiency loading an epoch, since it would lower the number of iterations. Further, we used two nested for loops in `_load_epoch`, where the inner one yields the actual batches. The purpose of the outer loop in this function is to prefetch multiple batches for shuffling -- otherwise, the shuffling won't have any effect on the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 ms ± 26.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "def one_epoch():\n",
    "    for batch_x, batch_y in batch_loader.load_train_epoch(\n",
    "            batch_size=100, shuffle_batch=True, prefetch_batches=4, \n",
    "            seed=123, onehot=True):\n",
    "        pass\n",
    "    \n",
    "% timeit one_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as we can see from the benchmark, prefetching multiple batches from the HDF5 database can speed up the loading of an epoch. Note that this could not always practicable (for example, when we are working with high-resolution images) due to memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Model using TensorFlow's `feed_dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrate how we can feed our minibatches into a TensorFlow graph using a TensorFlow session's `feed_dict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py:539: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-7678b02e171e>:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/venv/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# Architecture\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 256\n",
    "height, width = 28, 28\n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "##########################\n",
    "### GRAPH DEFINITION\n",
    "##########################\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    tf.set_random_seed(123)\n",
    "\n",
    "    # Input data\n",
    "    tf_x = tf.placeholder(tf.float32, [None, height, width, 1], name='features')\n",
    "    tf_x_flat = tf.reshape(tf_x, shape=[-1, height*width])\n",
    "    tf_y = tf.placeholder(tf.int32, [None, n_classes], name='targets')\n",
    "\n",
    "    # Model parameters\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([width*height, n_hidden_1], stddev=0.1)),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev=0.1)),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes], stddev=0.1))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Multilayer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(tf_x_flat, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=tf_y)\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train = optimizer.minimize(cost, name='train')\n",
    "\n",
    "    # Prediction\n",
    "    correct_prediction = tf.equal(tf.argmax(tf_y, 1), tf.argmax(out_layer, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network with Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | AvgCost: 0.463 | MbTrain/Valid ACC: 0.960/0.924\n",
      "Epoch: 002 | AvgCost: 0.219 | MbTrain/Valid ACC: 0.970/0.944\n",
      "Epoch: 003 | AvgCost: 0.164 | MbTrain/Valid ACC: 0.970/0.955\n",
      "Epoch: 004 | AvgCost: 0.132 | MbTrain/Valid ACC: 0.990/0.957\n",
      "Epoch: 005 | AvgCost: 0.111 | MbTrain/Valid ACC: 1.000/0.958\n",
      "Epoch: 006 | AvgCost: 0.094 | MbTrain/Valid ACC: 0.990/0.960\n",
      "Epoch: 007 | AvgCost: 0.083 | MbTrain/Valid ACC: 1.000/0.964\n",
      "Epoch: 008 | AvgCost: 0.073 | MbTrain/Valid ACC: 1.000/0.966\n",
      "Epoch: 009 | AvgCost: 0.065 | MbTrain/Valid ACC: 1.000/0.967\n",
      "Epoch: 010 | AvgCost: 0.056 | MbTrain/Valid ACC: 1.000/0.965\n",
      "Epoch: 011 | AvgCost: 0.050 | MbTrain/Valid ACC: 1.000/0.967\n",
      "Epoch: 012 | AvgCost: 0.044 | MbTrain/Valid ACC: 1.000/0.967\n",
      "Epoch: 013 | AvgCost: 0.039 | MbTrain/Valid ACC: 1.000/0.968\n",
      "Epoch: 014 | AvgCost: 0.035 | MbTrain/Valid ACC: 1.000/0.968\n",
      "Epoch: 015 | AvgCost: 0.031 | MbTrain/Valid ACC: 1.000/0.969\n",
      "Test ACC: 0.975\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### TRAINING & EVALUATION\n",
    "##########################\n",
    "\n",
    "batch_loader = BatchLoader(minibatches_path='./mnist_batches.h5', \n",
    "                           normalize=True)\n",
    "\n",
    "# preload small validation set\n",
    "# by unpacking the generator\n",
    "[valid_data] = batch_loader.load_validation_epoch(batch_size=5000, \n",
    "                                                   onehot=True)\n",
    "valid_x, valid_y = valid_data[0], valid_data[1]\n",
    "del valid_data\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "\n",
    "        n_batches = 0\n",
    "        for batch_x, batch_y in batch_loader.load_train_epoch(batch_size=batch_size, \n",
    "                                                              onehot=True,\n",
    "                                                              shuffle_batch=True,\n",
    "                                                              prefetch_batches=10,\n",
    "                                                              seed=epoch):\n",
    "            n_batches += 1\n",
    "            _, c = sess.run(['train', 'cost:0'], feed_dict={'features:0': batch_x,\n",
    "                                                            'targets:0': batch_y.astype(np.int)})\n",
    "            avg_cost += c\n",
    "        \n",
    "        train_acc = sess.run('accuracy:0', feed_dict={'features:0': batch_x,\n",
    "                                                      'targets:0': batch_y})\n",
    "        \n",
    "        valid_acc = sess.run('accuracy:0', feed_dict={'features:0': valid_x,\n",
    "                                                      'targets:0': valid_y})  \n",
    "        \n",
    "        print(\"Epoch: %03d | AvgCost: %.3f\" % (epoch + 1, avg_cost / n_batches), end=\"\")\n",
    "        print(\" | MbTrain/Valid ACC: %.3f/%.3f\" % (train_acc, valid_acc))\n",
    "        \n",
    "        \n",
    "    # imagine test set is too large to fit into memory:\n",
    "    test_acc, cnt = 0., 0\n",
    "    for test_x, test_y in batch_loader.load_test_epoch(batch_size=100, \n",
    "                                                       onehot=True):   \n",
    "        cnt += 1\n",
    "        acc = sess.run(accuracy, feed_dict={'features:0': test_x,\n",
    "                                            'targets:0': test_y})\n",
    "        test_acc += acc\n",
    "    print('Test ACC: %.3f' % (test_acc / cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
