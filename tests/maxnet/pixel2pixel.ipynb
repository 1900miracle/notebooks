{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel to Pixel Generative Adversarial Networks\n",
    "\n",
    "[Pixel to Pixel Generative Adversarial Networks](https://arxiv.org/abs/1611.07004) applies [Conditional Generative Adversarial Networks](https://arxiv.org/abs/1411.1784) as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. \n",
    "\n",
    "With pixel2pixel GAN, it is possible to train different type of image translation tasks with small datasets. In this tutorial, we will train on three image translation tasks: facades with 400 images from [CMP Facades dataset](http://cmp.felk.cvut.cz/%7Etylecr1/facade/), cityscapes with 2975 images from [Cityscapes training set](https://www.cityscapes-dataset.com/) and maps with 1096 training images scraped from Google Maps.\n",
    "\n",
    "For harder problems such as [edges2shoes](http://vision.cs.utexas.edu/projects/finegrained/utzap50k/) and [edges2handbags](https://github.com/junyanz/iGAN), it may be important to train on far larger datasets, which takes significantly more time. You can try them with [Multiple GPUs](http://gluon.mxnet.io/chapter07_distributed-learning/multiple-gpus-gluon.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Using TensorFlow backend.\n",
      "/srv/conda/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gopala KR \n",
      "last updated: 2018-03-01 \n",
      "\n",
      "CPython 3.6.4\n",
      "IPython 6.2.1\n",
      "\n",
      "watermark 1.6.0\n",
      "numpy 1.14.1\n",
      "matplotlib 2.1.2\n",
      "nltk 3.2.5\n",
      "sklearn 0.19.1\n",
      "tensorflow 1.5.0\n",
      "theano 1.0.1\n",
      "mxnet 1.2.0\n",
      "chainer 3.4.0\n",
      "seaborn 0.8.1\n",
      "keras 2.1.4\n",
      "tflearn n\u0007\n",
      "bokeh 0.12.14\n",
      "gensim 3.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "#load watermark\n",
    "%load_ext watermark\n",
    "%watermark -a 'Gopala KR' -u -d -v -p watermark,numpy,matplotlib,nltk,sklearn,tensorflow,theano,mxnet,chainer,seaborn,keras,tflearn,bokeh,gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import tarfile\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet.gluon import nn, utils\n",
    "from mxnet.gluon.nn import Dense, Activation, Conv2D, Conv2DTranspose, \\\n",
    "    BatchNorm, LeakyReLU, Flatten, HybridSequential, HybridBlock, Dropout\n",
    "from mxnet import autograd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "use_gpu = True\n",
    "ctx = mx.gpu() if use_gpu else mx.cpu()\n",
    "\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "lambda1 = 100\n",
    "\n",
    "pool_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Preprocess Dataset\n",
    "\n",
    "We first train on facades dataset. We need to crop images to input images and output images. Notice that pixel2pixel GAN is capable to train these tasks bidirectional. You can set ```is-reversed=True``` to switch input and output image patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'facades'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first resize images to size 512 * 256. Then normalize image pixel values to be between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading facades.tar.gz from https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "img_wd = 256\n",
    "img_ht = 256\n",
    "train_img_path = '%s/train' % (dataset)\n",
    "val_img_path = '%s/val' % (dataset)\n",
    "\n",
    "def download_data(dataset):\n",
    "    if not os.path.exists(dataset):\n",
    "        url = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/%s.tar.gz' % (dataset)\n",
    "        os.mkdir(dataset)\n",
    "        data_file = utils.download(url)\n",
    "        with tarfile.open(data_file) as tar:\n",
    "            tar.extractall(path='.')\n",
    "        os.remove(data_file)\n",
    "\n",
    "def load_data(path, batch_size, is_reversed=False):\n",
    "    img_in_list = []\n",
    "    img_out_list = []\n",
    "    for path, _, fnames in os.walk(path):\n",
    "        for fname in fnames:\n",
    "            if not fname.endswith('.jpg'):\n",
    "                continue\n",
    "            img = os.path.join(path, fname)\n",
    "            img_arr = mx.image.imread(img).astype(np.float32)/127.5 - 1\n",
    "            img_arr = mx.image.imresize(img_arr, img_wd * 2, img_ht)\n",
    "            # Crop input and output images\n",
    "            img_arr_in, img_arr_out = [mx.image.fixed_crop(img_arr, 0, 0, img_wd, img_ht),\n",
    "                                       mx.image.fixed_crop(img_arr, img_wd, 0, img_wd, img_ht)]\n",
    "            img_arr_in, img_arr_out = [nd.transpose(img_arr_in, (2,0,1)), \n",
    "                                       nd.transpose(img_arr_out, (2,0,1))]\n",
    "            img_arr_in, img_arr_out = [img_arr_in.reshape((1,) + img_arr_in.shape), \n",
    "                                       img_arr_out.reshape((1,) + img_arr_out.shape)]\n",
    "            img_in_list.append(img_arr_out if is_reversed else img_arr_in)\n",
    "            img_out_list.append(img_arr_in if is_reversed else img_arr_out)\n",
    "\n",
    "    return mx.io.NDArrayIter(data=[nd.concat(*img_in_list, dim=0), nd.concat(*img_out_list, dim=0)], \n",
    "                             batch_size=batch_size)\n",
    "\n",
    "download_data(dataset)\n",
    "train_data = load_data(train_img_path, batch_size, is_reversed=True)\n",
    "val_data = load_data(val_img_path, batch_size, is_reversed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize 4 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(img_arr):\n",
    "    plt.imshow(((img_arr.asnumpy().transpose(1, 2, 0) + 1.0) * 127.5).astype(np.uint8))\n",
    "    plt.axis('off')\n",
    "\n",
    "def preview_train_data():\n",
    "    img_in_list, img_out_list = train_data.next().data\n",
    "    for i in range(4):\n",
    "        plt.subplot(2,4,i+1)\n",
    "        visualize(img_in_list[i])\n",
    "        plt.subplot(2,4,i+5)\n",
    "        visualize(img_out_list[i])\n",
    "    plt.show()\n",
    "    \n",
    "preview_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the networks\n",
    "\n",
    "Both generator and discriminator use modules of the form convolution-BatchNorm-ReLu. \n",
    "\n",
    "The key for generator is U-net architecture adding skip connections which shuttle low-level infomation shared between input and output images across net.\n",
    "![](../img/Pixel2pixel-Unet.png \"Generator Architecture\")\n",
    "\n",
    "PatchGAN – that only penalizes structure at the scale of patches is applied as disciminator architecture. This discriminator tries to classify if each N × N patch in an image is real or fake. We run this discriminator convolutionally across the image, averaging all responses to provide the ultimate output of netD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Unet generator skip block\n",
    "class UnetSkipUnit(HybridBlock):\n",
    "    def __init__(self, inner_channels, outer_channels, inner_block=None, innermost=False, outermost=False,\n",
    "                 use_dropout=False, use_bias=False):\n",
    "        super(UnetSkipUnit, self).__init__()\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.outermost = outermost\n",
    "            en_conv = Conv2D(channels=inner_channels, kernel_size=4, strides=2, padding=1,\n",
    "                             in_channels=outer_channels, use_bias=use_bias)\n",
    "            en_relu = LeakyReLU(alpha=0.2)\n",
    "            en_norm = BatchNorm(momentum=0.1, in_channels=inner_channels)\n",
    "            de_relu = Activation(activation='relu')\n",
    "            de_norm = BatchNorm(momentum=0.1, in_channels=outer_channels)\n",
    "\n",
    "            if innermost:\n",
    "                de_conv = Conv2DTranspose(channels=outer_channels, kernel_size=4, strides=2, padding=1,\n",
    "                                          in_channels=inner_channels, use_bias=use_bias)\n",
    "                encoder = [en_relu, en_conv]\n",
    "                decoder = [de_relu, de_conv, de_norm]\n",
    "                model = encoder + decoder\n",
    "            elif outermost:\n",
    "                de_conv = Conv2DTranspose(channels=outer_channels, kernel_size=4, strides=2, padding=1,\n",
    "                                          in_channels=inner_channels * 2)\n",
    "                encoder = [en_conv]\n",
    "                decoder = [de_relu, de_conv, Activation(activation='tanh')]\n",
    "                model = encoder + [inner_block] + decoder\n",
    "            else:\n",
    "                de_conv = Conv2DTranspose(channels=outer_channels, kernel_size=4, strides=2, padding=1,\n",
    "                                          in_channels=inner_channels * 2, use_bias=use_bias)\n",
    "                encoder = [en_relu, en_conv, en_norm]\n",
    "                decoder = [de_relu, de_conv, de_norm]\n",
    "                model = encoder + [inner_block] + decoder\n",
    "            if use_dropout:\n",
    "                model += [Dropout(rate=0.5)]\n",
    "        \n",
    "            self.model = HybridSequential()\n",
    "            with self.model.name_scope():\n",
    "                for block in model:\n",
    "                    self.model.add(block)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return F.concat(self.model(x), x, dim=1)\n",
    "\n",
    "# Define Unet generator\n",
    "class UnetGenerator(HybridBlock):\n",
    "    def __init__(self, in_channels, num_downs, ngf=64, use_dropout=True):\n",
    "        super(UnetGenerator, self).__init__()\n",
    "\n",
    "        #Build unet generator structure\n",
    "        unet = UnetSkipUnit(ngf * 8, ngf * 8, innermost=True)\n",
    "        for _ in range(num_downs - 5):\n",
    "            unet = UnetSkipUnit(ngf * 8, ngf * 8, unet, use_dropout=use_dropout)\n",
    "        unet = UnetSkipUnit(ngf * 8, ngf * 4, unet)\n",
    "        unet = UnetSkipUnit(ngf * 4, ngf * 2, unet)\n",
    "        unet = UnetSkipUnit(ngf * 2, ngf * 1, unet)\n",
    "        unet = UnetSkipUnit(ngf, in_channels, unet, outermost=True)\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.model = unet\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the PatchGAN discriminator\n",
    "class Discriminator(HybridBlock):\n",
    "    def __init__(self, in_channels, ndf=64, n_layers=3, use_sigmoid=False, use_bias=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.model = HybridSequential()\n",
    "            kernel_size = 4\n",
    "            padding = int(np.ceil((kernel_size - 1)/2))\n",
    "            self.model.add(Conv2D(channels=ndf, kernel_size=kernel_size, strides=2,\n",
    "                                  padding=padding, in_channels=in_channels))\n",
    "            self.model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "            nf_mult = 1\n",
    "            for n in range(1, n_layers):\n",
    "                nf_mult_prev = nf_mult\n",
    "                nf_mult = min(2 ** n, 8)\n",
    "                self.model.add(Conv2D(channels=ndf * nf_mult, kernel_size=kernel_size, strides=2,\n",
    "                                      padding=padding, in_channels=ndf * nf_mult_prev,\n",
    "                                      use_bias=use_bias))\n",
    "                self.model.add(BatchNorm(momentum=0.1, in_channels=ndf * nf_mult))\n",
    "                self.model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n_layers, 8)\n",
    "            self.model.add(Conv2D(channels=ndf * nf_mult, kernel_size=kernel_size, strides=1,\n",
    "                                  padding=padding, in_channels=ndf * nf_mult_prev,\n",
    "                                  use_bias=use_bias))\n",
    "            self.model.add(BatchNorm(momentum=0.1, in_channels=ndf * nf_mult))\n",
    "            self.model.add(LeakyReLU(alpha=0.2))\n",
    "            self.model.add(Conv2D(channels=1, kernel_size=kernel_size, strides=1,\n",
    "                                  padding=padding, in_channels=ndf * nf_mult))\n",
    "            if use_sigmoid:\n",
    "                self.model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = self.model(x)\n",
    "        #print(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct networks, Initialize parameters, Setup Loss Function and Optimizer\n",
    "We use binary cross entropy and L1 loss as loss functions. L1 loss can be used to capture low frequencies in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_init(param):\n",
    "    if param.name.find('conv') != -1:\n",
    "        if param.name.find('weight') != -1:\n",
    "            param.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "        else:\n",
    "            param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "    elif param.name.find('batchnorm') != -1:\n",
    "        param.initialize(init=mx.init.Zero(), ctx=ctx)\n",
    "        # Initialize gamma from normal distribution with mean 1 and std 0.02\n",
    "        if param.name.find('gamma') != -1:\n",
    "            param.set_data(nd.random_normal(1, 0.02, param.data().shape))\n",
    "\n",
    "def network_init(net):\n",
    "    for param in net.collect_params().values():\n",
    "        param_init(param)\n",
    "\n",
    "def set_network():\n",
    "    # Pixel2pixel networks\n",
    "    netG = UnetGenerator(in_channels=3, num_downs=8)\n",
    "    netD = Discriminator(in_channels=6)\n",
    "\n",
    "    # Initialize parameters\n",
    "    network_init(netG)\n",
    "    network_init(netD)\n",
    "\n",
    "    # trainer for the generator and the discriminator\n",
    "    trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})\n",
    "    trainerD = gluon.Trainer(netD.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})\n",
    "    \n",
    "    return netG, netD, trainerG, trainerD\n",
    "\n",
    "# Loss\n",
    "GAN_loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "L1_loss = gluon.loss.L1Loss()\n",
    "\n",
    "netG, netD, trainerG, trainerD = set_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image pool for discriminator\n",
    "We use history image pool to help discriminator memorize history errors instead of just comparing current real input and fake output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        ret_imgs = []\n",
    "        for i in range(images.shape[0]):\n",
    "            image = nd.expand_dims(images[i], axis=0)\n",
    "            if self.num_imgs < self.pool_size:\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                ret_imgs.append(image)\n",
    "            else:\n",
    "                p = nd.random_uniform(0, 1, shape=(1,)).asscalar()\n",
    "                if p > 0.5:\n",
    "                    random_id = nd.random_uniform(0, self.pool_size - 1, shape=(1,)).astype(np.uint8).asscalar()\n",
    "                    tmp = self.images[random_id].copy()\n",
    "                    self.images[random_id] = image\n",
    "                    ret_imgs.append(tmp)\n",
    "                else:\n",
    "                    ret_imgs.append(image)\n",
    "        ret_imgs = nd.concat(*ret_imgs, dim=0)\n",
    "        return ret_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "We recommend to use gpu to boost training. After a few epochs, we can see images silimar to building structure are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "\n",
    "def facc(label, pred):\n",
    "        pred = pred.ravel()\n",
    "        label = label.ravel()\n",
    "        return ((pred > 0.5) == label).mean()\n",
    "\n",
    "def train():\n",
    "    image_pool = ImagePool(pool_size)\n",
    "    metric = mx.metric.CustomMetric(facc)\n",
    "\n",
    "    stamp =  datetime.now().strftime('%Y_%m_%d-%H_%M')\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        btic = time.time()\n",
    "        train_data.reset()\n",
    "        iter = 0\n",
    "        for batch in train_data:\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x, y)) + log(1 - D(x, G(x, z)))\n",
    "            ###########################\n",
    "            real_in = batch.data[0].as_in_context(ctx)\n",
    "            real_out = batch.data[1].as_in_context(ctx)\n",
    "\n",
    "            fake_out = netG(real_in)\n",
    "            fake_concat = image_pool.query(nd.concat(real_in, fake_out, dim=1))\n",
    "            with autograd.record():\n",
    "                # Train with fake image\n",
    "                # Use image pooling to utilize history images\n",
    "                output = netD(fake_concat)\n",
    "                fake_label = nd.zeros(output.shape, ctx=ctx)\n",
    "                errD_fake = GAN_loss(output, fake_label)\n",
    "                metric.update([fake_label,], [output,])\n",
    "            \n",
    "                # Train with real image\n",
    "                real_concat = nd.concat(real_in, real_out, dim=1)\n",
    "                output = netD(real_concat)\n",
    "                real_label = nd.ones(output.shape, ctx=ctx)\n",
    "                errD_real = GAN_loss(output, real_label)\n",
    "                errD = (errD_real + errD_fake) * 0.5\n",
    "                errD.backward()\n",
    "                metric.update([real_label,], [output,])\n",
    "\n",
    "            trainerD.step(batch.data[0].shape[0])\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(x, G(x, z))) - lambda1 * L1(y, G(x, z))\n",
    "            ###########################\n",
    "            with autograd.record():\n",
    "                fake_out = netG(real_in)\n",
    "                fake_concat = nd.concat(real_in, fake_out, dim=1)\n",
    "                output = netD(fake_concat)\n",
    "                real_label = nd.ones(output.shape, ctx=ctx)\n",
    "                errG = GAN_loss(output, real_label) + L1_loss(real_out, fake_out) * lambda1\n",
    "                errG.backward()\n",
    "\n",
    "            trainerG.step(batch.data[0].shape[0])\n",
    "\n",
    "            # Print log infomation every ten batches\n",
    "            if iter % 10 == 0:\n",
    "                name, acc = metric.get()\n",
    "                logging.info('speed: {} samples/s'.format(batch_size / (time.time() - btic)))\n",
    "                logging.info('discriminator loss = %f, generator loss = %f, binary training acc = %f at iter %d epoch %d' \n",
    "                         %(nd.mean(errD).asscalar(), \n",
    "                           nd.mean(errG).asscalar(), acc, iter, epoch))\n",
    "            iter = iter + 1\n",
    "            btic = time.time()\n",
    "\n",
    "        name, acc = metric.get()\n",
    "        metric.reset()\n",
    "        logging.info('\\nbinary training acc at epoch %d: %s=%f' % (epoch, name, acc))\n",
    "        logging.info('time: %f' % (time.time() - tic))\n",
    "\n",
    "        # Visualize one generated image for each epoch\n",
    "        fake_img = fake_out[0]\n",
    "        visualize(fake_img)\n",
    "        plt.show()\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Generate images with generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result():\n",
    "    num_image = 4\n",
    "    img_in_list, img_out_list = val_data.next().data\n",
    "    for i in range(num_image):\n",
    "        img_in = nd.expand_dims(img_in_list[i], axis=0)\n",
    "        plt.subplot(2,4,i+1)\n",
    "        visualize(img_in[0])\n",
    "        img_out = netG(img_in.as_in_context(ctx))\n",
    "        plt.subplot(2,4,i+5)\n",
    "        visualize(img_out[0])\n",
    "    plt.show()\n",
    "    \n",
    "print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other dataset experiments\n",
    "Run experiments on cityscapes and maps datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['cityscapes', 'maps']\n",
    "is_reversed = False\n",
    "batch_size = 64\n",
    "\n",
    "for dataset in datasets:\n",
    "    train_img_path = '%s/train' % (dataset)\n",
    "    val_img_path = '%s/val' % (dataset)\n",
    "    download_data(dataset)\n",
    "    train_data = load_data(train_img_path, batch_size, is_reversed=is_reversed)\n",
    "    val_data = load_data(val_img_path, batch_size, is_reversed=is_reversed)\n",
    "\n",
    "    print(\"Preview %s training data:\" % (dataset))\n",
    "    preview_train_data()\n",
    "\n",
    "    netG, netD, trainerG, trainerD = set_network()\n",
    "    train()\n",
    "    \n",
    "    print(\"Training result for %s\" % (dataset))\n",
    "    print_result()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "CMP Facades dataset:\n",
    "@INPROCEEDINGS{\n",
    "  Tylecek13,\n",
    "  author = {Radim Tyle{\\v c}ek, Radim {\\v S}{\\' a}ra},\n",
    "  title = {Spatial Pattern Templates for Recognition of Objects with Regular Structure},\n",
    "  booktitle = {Proc. GCPR},\n",
    "  year = {2013},\n",
    "  address = {Saarbrucken, Germany},\n",
    "}\n",
    "\n",
    "Cityscapes training set:\n",
    "@inproceedings{Cordts2016Cityscapes,\n",
    "    title={The Cityscapes Dataset for Semantic Urban Scene Understanding},\n",
    "    author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and      Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},\n",
    "    booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "    year={2016}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
