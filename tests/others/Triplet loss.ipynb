{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face verification\n",
    "\n",
    "### Goals\n",
    "- train a network for face similarity using siamese networks\n",
    "- train a network for face similarity using triplet loss\n",
    "\n",
    "the architecture is as follows:\n",
    "\n",
    "_image_\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- We will be using Labeled Faces in the Wild (LFW) dataset available openly at _url_\n",
    "- For computing purposes, we'll only restrict ourselves to a subpart of the dataset. You're welcome to train on the whole dataset on GPU\n",
    "- We will also load pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Gopala KR \n",
      "last updated: 2018-03-04 \n",
      "\n",
      "CPython 3.6.3\n",
      "IPython 6.2.1\n",
      "\n",
      "watermark 1.6.0\n",
      "numpy 1.14.1\n",
      "matplotlib 2.1.2\n",
      "nltk 3.2.5\n"
     ]
    }
   ],
   "source": [
    "#load watermark\n",
    "%load_ext watermark\n",
    "%watermark -a 'Gopala KR' -u -d -v -p watermark,numpy,matplotlib,nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Concatenate, merge, Lambda, Dot\n",
    "from keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D, Flatten, Dropout\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the dataset\n",
    "\n",
    "The dataset consists of folders corresponding to each identity. The folder name is the name of the person.\n",
    "We map each class (identity) to an integer id, and build mappings as dictionaries `name_to_classid` and `classid_to_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"lfw-a/lfw/\"\n",
    "PATH = \"lfw/lfw-deepfunneled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 5749\n"
     ]
    }
   ],
   "source": [
    "dirs = sorted(os.listdir(PATH))\n",
    "name_to_classid = {d:i for i,d in enumerate(dirs)}\n",
    "classid_to_name = {v:k for k,v in name_to_classid.items()}\n",
    "num_classes = len(name_to_classid)\n",
    "print(\"number of classes: \"+str(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each directory, there is one or more images corresponding to the identity. We map each image path with an integer id, then build a few dictionaries:\n",
    "- mappings from imagepath and image id: `path_to_id` and `id_to_path`\n",
    "- mappings from class id to image ids: `classid_to_ids` and `id_to_classid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all directories\n",
    "img_paths = {c:[directory + \"/\" + img for img in sorted(os.listdir(PATH+directory))] \n",
    "             for directory,c in name_to_classid.items()}\n",
    "\n",
    "# retrieve all images\n",
    "all_images_path = []\n",
    "for img_list in img_paths.values():\n",
    "    all_images_path += img_list\n",
    "\n",
    "# map to integers\n",
    "path_to_id = {v:k for k,v in enumerate(all_images_path)}\n",
    "id_to_path = {v:k for k,v in path_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 5,\n",
       " 7: 5,\n",
       " 8: 5,\n",
       " 9: 6,\n",
       " 10: 7,\n",
       " 11: 7,\n",
       " 12: 8}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build mappings between images and class\n",
    "classid_to_ids = {k:[path_to_id[path] for path in v] for k,v in img_paths.items()}\n",
    "id_to_classid = {v:c for c,imgs in classid_to_ids.items() for v in imgs}\n",
    "dict(list(id_to_classid.items())[0:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following histogram shows the number of images per class: there are many classes with only one image. \n",
    "These classes are useful as negatives, only as we can't make a positive pair with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('George_W_Bush', 530),\n",
       " ('Colin_Powell', 236),\n",
       " ('Tony_Blair', 144),\n",
       " ('Donald_Rumsfeld', 121),\n",
       " ('Gerhard_Schroeder', 109),\n",
       " ('Ariel_Sharon', 77),\n",
       " ('Hugo_Chavez', 71),\n",
       " ('Junichiro_Koizumi', 60),\n",
       " ('Jean_Chretien', 55),\n",
       " ('John_Ashcroft', 53)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(classid_to_name[x], len(classid_to_ids[x])) for x in np.argsort([len(v) for k,v in classid_to_ids.items()])[::-1][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAFM5JREFUeJzt3X2QXfV93/H3xxIGGzvmacMQSa2YRE2KPRNBtkBK6nGhBgEei3QSF6a1NQwzcmZEi9tME/A/xHaYwTOJST1jM6NYikVqo6pgDxpbNVaB1PUfPKywDAiZsuEhkiqjjQXYlAZH+Ns/7k/0Gq/Ye6XdvWvO+zVzZ8/5nt8593s0sJ89D/eeVBWSpO55y6gbkCSNhgEgSR1lAEhSRxkAktRRBoAkdZQBIEkdZQBIUkcZAJLUUQaAJHXU4lE38EZOO+20Wr58+ajbkKSfKzt27PjbqhqbadyCDoDly5czMTEx6jYk6edKkmcHGecpIEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeqogT8JnGQRMAHsq6oPJDkT2AycCuwAPlxVP05yPHAb8BvAD4B/VVXPtG3cAFwDvAr8u6q6ezZ35vWWX//1udz8UXvm5stH3YIkDXUEcB2wu2/+08AtVfUrwPP0frHTfj7f6re0cSQ5C7gSeDewCvh8CxVJ0ggMFABJlgKXA19o8wEuBO5oQzYBV7Tp1W2etvyiNn41sLmqXqmqp4FJ4NzZ2AlJ0vAGPQL4M+APgJ+0+VOBF6rqUJvfCyxp00uAPQBt+Ytt/Gv1adZ5TZK1SSaSTExNTQ2xK5KkYcwYAEk+AByoqh3z0A9Vtb6qxqtqfGxsxm8zlSQdpUEuAl8AfDDJZcAJwC8A/wk4Kcni9lf+UmBfG78PWAbsTbIYeBe9i8GH64f1ryNJmmczHgFU1Q1VtbSqltO7iHtvVf1r4D7gd9qwNcBdbXprm6ctv7eqqtWvTHJ8u4NoBfDgrO2JJGkox/JAmD8ENif5Y+A7wIZW3wD8ZZJJ4CC90KCqdiXZAjwOHALWVdWrx/D+kqRjMFQAVNVfAX/Vpp9imrt4qurvgN89wvo3ATcN26Qkafb5SWBJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowZ5KPwJSR5M8t0ku5J8otW/mOTpJDvba2WrJ8lnk0wmeSTJOX3bWpPkyfZac6T3lCTNvUGeCPYKcGFVvZTkOODbSf5bW/Yfq+qO142/lN7zflcA5wG3AuclOQW4ERgHCtiRZGtVPT8bOyJJGs4gD4WvqnqpzR7XXvUGq6wGbmvr3Q+clOQM4BJge1UdbL/0twOrjq19SdLRGugaQJJFSXYCB+j9En+gLbqpnea5JcnxrbYE2NO3+t5WO1JdkjQCAwVAVb1aVSuBpcC5Sd4D3AD8GvBPgFOAP5yNhpKsTTKRZGJqamo2NilJmsZQdwFV1QvAfcCqqtrfTvO8AvwFcG4btg9Y1rfa0lY7Uv3177G+qsaranxsbGyY9iRJQxjkLqCxJCe16bcB7we+187rkyTAFcBjbZWtwEfa3UDnAy9W1X7gbuDiJCcnORm4uNUkSSMwyF1AZwCbkiyiFxhbquprSe5NMgYE2An8Xhu/DbgMmAReBq4GqKqDST4FPNTGfbKqDs7erkiShjFjAFTVI8DZ09QvPML4AtYdYdlGYOOQPUqS5oCfBJakjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6apBHQp6Q5MEk302yK8knWv3MJA8kmUzyX5K8tdWPb/OTbfnyvm3d0OpPJLlkrnZKkjSzQY4AXgEurKpfB1YCq9qzfj8N3FJVvwI8D1zTxl8DPN/qt7RxJDkLuBJ4N7AK+Hx7zKQkaQRmDIDqeanNHtdeBVwI3NHqm+g9GB5gdZunLb+oPTh+NbC5ql6pqqfpPTP43FnZC0nS0Aa6BpBkUZKdwAFgO/DXwAtVdagN2QssadNLgD0AbfmLwKn99WnWkSTNs4ECoKperaqVwFJ6f7X/2lw1lGRtkokkE1NTU3P1NpLUeUPdBVRVLwD3Ab8JnJRkcVu0FNjXpvcBywDa8ncBP+ivT7NO/3usr6rxqhofGxsbpj1J0hAGuQtoLMlJbfptwPuB3fSC4HfasDXAXW16a5unLb+3qqrVr2x3CZ0JrAAenK0dkSQNZ/HMQzgD2NTu2HkLsKWqvpbkcWBzkj8GvgNsaOM3AH+ZZBI4SO/OH6pqV5ItwOPAIWBdVb06u7sjSRrUjAFQVY8AZ09Tf4pp7uKpqr8DfvcI27oJuGn4NiVJs81PAktSRxkAktRRBoAkdZQBIEkdZQBIUkcZAJLUUQaAJHWUASBJHWUASFJHGQCS1FEGgCR1lAEgSR1lAEhSRxkAktRRBoAkdZQBIEkdZQBIUkcN8kzgZUnuS/J4kl1Jrmv1P0qyL8nO9rqsb50bkkwmeSLJJX31Va02meT6udklSdIgBnkm8CHg96vq4STvBHYk2d6W3VJVf9I/OMlZ9J4D/G7gl4D/nuQftcWfo/dQ+b3AQ0m2VtXjs7EjkqThDPJM4P3A/jb9oyS7gSVvsMpqYHNVvQI83R4Of/jZwZPtWcIk2dzGGgCSNAJDXQNIspzeA+IfaKVrkzySZGOSk1ttCbCnb7W9rXakuiRpBAYOgCTvAO4EPlZVPwRuBX4ZWEnvCOFPZ6OhJGuTTCSZmJqamo1NSpKmMVAAJDmO3i//L1XVVwCq6rmqerWqfgL8Of//NM8+YFnf6ktb7Uj1n1JV66tqvKrGx8bGht0fSdKABrkLKMAGYHdVfaavfkbfsN8GHmvTW4Erkxyf5ExgBfAg8BCwIsmZSd5K70Lx1tnZDUnSsAa5C+gC4MPAo0l2ttrHgauSrAQKeAb4KEBV7Uqyhd7F3UPAuqp6FSDJtcDdwCJgY1XtmsV9kSQNYZC7gL4NZJpF295gnZuAm6apb3uj9SRJ88dPAktSRxkAktRRBoAkdZQBIEkdZQBIUkcZAJLUUQaAJHWUASBJHWUASFJHGQCS1FEGgCR1lAEgSR1lAEhSRxkAktRRBoAkdZQBIEkdNcgjIZcluS/J40l2Jbmu1U9Jsj3Jk+3nya2eJJ9NMpnkkSTn9G1rTRv/ZJI1c7dbkqSZDHIEcAj4/ao6CzgfWJfkLOB64J6qWgHc0+YBLqX3HOAVwFrgVugFBnAjcB69B8jfeDg0JEnzb8YAqKr9VfVwm/4RsBtYAqwGNrVhm4Ar2vRq4LbquR84qT1A/hJge1UdrKrnge3AqlndG0nSwIa6BpBkOXA28ABwelXtb4u+D5zeppcAe/pW29tqR6pLkkZg4ABI8g7gTuBjVfXD/mVVVUDNRkNJ1iaZSDIxNTU1G5uUJE1joABIchy9X/5fqqqvtPJz7dQO7eeBVt8HLOtbfWmrHan+U6pqfVWNV9X42NjYMPsiSRrCIHcBBdgA7K6qz/Qt2gocvpNnDXBXX/0j7W6g84EX26miu4GLk5zcLv5e3GqSpBFYPMCYC4APA48m2dlqHwduBrYkuQZ4FvhQW7YNuAyYBF4GrgaoqoNJPgU81MZ9sqoOzspeSJKGNmMAVNW3gRxh8UXTjC9g3RG2tRHYOEyDkqS54SeBJamjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowZ5JvDGJAeSPNZX+6Mk+5LsbK/L+pbdkGQyyRNJLumrr2q1ySTXz/6uSJKGMcgRwBeBVdPUb6mqle21DSDJWcCVwLvbOp9PsijJIuBzwKXAWcBVbawkaUQGeSbwt5IsH3B7q4HNVfUK8HSSSeDctmyyqp4CSLK5jX186I4lSbPiWK4BXJvkkXaK6ORWWwLs6Ruzt9WOVP8ZSdYmmUgyMTU1dQztSZLeyNEGwK3ALwMrgf3An85WQ1W1vqrGq2p8bGxstjYrSXqdGU8BTaeqnjs8neTPga+12X3Asr6hS1uNN6hLkkbgqI4AkpzRN/vbwOE7hLYCVyY5PsmZwArgQeAhYEWSM5O8ld6F4q1H37Yk6VjNeASQ5HbgfcBpSfYCNwLvS7ISKOAZ4KMAVbUryRZ6F3cPAeuq6tW2nWuBu4FFwMaq2jXreyNJGtggdwFdNU15wxuMvwm4aZr6NmDbUN1JkuaMnwSWpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOmrGAEiyMcmBJI/11U5Jsj3Jk+3nya2eJJ9NMpnkkSTn9K2zpo1/MsmaudkdSdKgBjkC+CKw6nW164F7qmoFcE+bB7iU3nOAVwBrgVuhFxj0HiV5HnAucOPh0JAkjcaMAVBV3wIOvq68GtjUpjcBV/TVb6ue+4GT2gPkLwG2V9XBqnoe2M7PhookaR4d7TWA06tqf5v+PnB6m14C7Okbt7fVjlSXJI3IMV8ErqoCahZ6ASDJ2iQTSSampqZma7OSpNc52gB4rp3aof080Or7gGV945a22pHqP6Oq1lfVeFWNj42NHWV7kqSZHG0AbAUO38mzBrirr/6RdjfQ+cCL7VTR3cDFSU5uF38vbjVJ0ogsnmlAktuB9wGnJdlL726em4EtSa4BngU+1IZvAy4DJoGXgasBqupgkk8BD7Vxn6yq119YliTNoxkDoKquOsKii6YZW8C6I2xnI7BxqO4kSXPGTwJLUkcZAJLUUQaAJHWUASBJHWUASFJHGQCS1FEz3gaq2bf8+q+PuoVpPXPz5aNuQdI88ghAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkKSOOqYASPJMkkeT7Ewy0WqnJNme5Mn28+RWT5LPJplM8kiSc2ZjByRJR2c2jgD+eVWtrKrxNn89cE9VrQDuafMAlwIr2mstcOssvLck6SjNxSmg1cCmNr0JuKKvflv13A+clOSMOXh/SdIAjjUACvhmkh1J1rba6VW1v01/Hzi9TS8B9vStu7fVfkqStUkmkkxMTU0dY3uSpCM51q+D/q2q2pfkF4HtSb7Xv7CqKkkNs8GqWg+sBxgfHx9qXUnS4I7pCKCq9rWfB4CvAucCzx0+tdN+HmjD9wHL+lZf2mqSpBE46gBIcmKSdx6eBi4GHgO2AmvasDXAXW16K/CRdjfQ+cCLfaeKJEnz7FhOAZ0OfDXJ4e18uaq+keQhYEuSa4BngQ+18duAy4BJ4GXg6mN4b0nSMTrqAKiqp4Bfn6b+A+CiaeoFrDva95MkzS4/CSxJHWUASFJHGQCS1FEGgCR1lAEgSR11rJ8E1pvI8uu/PuoWpvXMzZePugXpTckjAEnqKANAkjrKAJCkjjIAJKmjvAisBc+L09Lc8AhAkjrKAJCkjjIAJKmjvAYgHSWvTejnnUcAktRR8x4ASVYleSLJZJLr5/v9JUk983oKKMki4HPA+4G9wENJtlbV4/PZh/Rm5qmp4XT532u+rwGcC0y2x0mSZDOwGjAApDe5hfqLtsvm+xTQEmBP3/zeVpMkzbMFdxdQkrXA2jb7UpInjmFzpwF/e+xdzTr7Go59Dce+hrMg+8qnj6mvfzjIoPkOgH3Asr75pa32mqpaD6yfjTdLMlFV47OxrdlkX8Oxr+HY13C63Nd8nwJ6CFiR5MwkbwWuBLbOcw+SJOb5CKCqDiW5FrgbWARsrKpd89mDJKln3q8BVNU2YNs8vd2snEqaA/Y1HPsajn0Np7N9parm+j0kSQuQXwUhSR31pgyAJBuTHEjy2Kh7OSzJsiT3JXk8ya4k1426J4AkJyR5MMl3W1+fGHVP/ZIsSvKdJF8bdS+HJXkmyaNJdiaZGHU/hyU5KckdSb6XZHeS3xx1TwBJfrX9Wx1+/TDJxxZAX/++/Tf/WJLbk5ww6p4AklzXeto11/9Ob8pTQEneC7wE3FZV7xl1PwBJzgDOqKqHk7wT2AFcMeqvwUgS4MSqeinJccC3geuq6v5R9nVYkv8AjAO/UFUfGHU/0AsAYLyqFtS940k2Af+zqr7Q7rJ7e1W9MOq++rWvg9kHnFdVz46wjyX0/ls/q6r+b5ItwLaq+uKoemp9vQfYTO9bE34MfAP4vaqanIv3e1MeAVTVt4CDo+6jX1Xtr6qH2/SPgN0sgE9BV89Lbfa49loQfxUkWQpcDnxh1L0sdEneBbwX2ABQVT9eaL/8m4uAvx7lL/8+i4G3JVkMvB343yPuB+AfAw9U1ctVdQj4H8C/nKs3e1MGwEKXZDlwNvDAaDvpaadZdgIHgO1VtSD6Av4M+APgJ6Nu5HUK+GaSHe2T6wvBmcAU8BftlNkXkpw46qamcSVw+6ibqKp9wJ8AfwPsB16sqm+OtisAHgP+WZJTk7wduIyf/vDsrDIA5lmSdwB3Ah+rqh+Ouh+Aqnq1qlbS+2T2ue0wdKSSfAA4UFU7Rt3LNH6rqs4BLgXWtVOOo7YYOAe4tarOBv4PsKC+br2dlvog8F8XQC8n0/siyjOBXwJOTPJvRtsVVNVu4NPAN+md/tkJvDpX72cAzKN2jv1O4EtV9ZVR9/N67ZTBfcCqUfcCXAB8sJ1v3wxcmOQ/j7alnvbXI1V1APgqvfO1o7YX2Nt39HYHvUBYSC4FHq6q50bdCPAvgKeraqqq/h74CvBPR9wTAFW1oap+o6reCzwP/K+5ei8DYJ60i60bgN1V9ZlR93NYkrEkJ7Xpt9F7VsP3RtsVVNUNVbW0qpbTO21wb1WN/C+0JCe2i/i0UywX0ztsH6mq+j6wJ8mvttJFLLyvWb+KBXD6p/kb4Pwkb2//b15E77rcyCX5xfbzH9A7///luXqvBfdtoLMhye3A+4DTkuwFbqyqDaPtiguADwOPtvPtAB9vn4wepTOATe3ujLcAW6pqwdxyuQCdDny19zuDxcCXq+obo23pNf8W+FI71fIUcPWI+3lNC8v3Ax8ddS8AVfVAkjuAh4FDwHdYOJ8IvjPJqcDfA+vm8mL+m/I2UEnSzDwFJEkdZQBIUkcZAJLUUQaAJHWUASBJHWUASFJHGQCS1FEGgCR11P8DFf06j0OHdhMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c39832358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(v) for k,v in classid_to_ids.items()], bins=range(1,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('George_W_Bush', 530),\n",
       " ('Colin_Powell', 236),\n",
       " ('Tony_Blair', 144),\n",
       " ('Donald_Rumsfeld', 121),\n",
       " ('Gerhard_Schroeder', 109),\n",
       " ('Ariel_Sharon', 77),\n",
       " ('Hugo_Chavez', 71),\n",
       " ('Junichiro_Koizumi', 60),\n",
       " ('Jean_Chretien', 55),\n",
       " ('John_Ashcroft', 53)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(classid_to_name[x], len(classid_to_ids[x])) for x in np.argsort([len(v) for k,v in classid_to_ids.items()])[::-1][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### siamese nets\n",
    "\n",
    "A siamese net takes as input two images $x_1$ and $x_2$ and outputs a single value which corresponds to the similarity between $x_1$ and $x_2$.\n",
    "\n",
    "In order to train such a system, one has to build positive and negative pairs for the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pos_pairs_for_id(classid, max_num=50):\n",
    "    imgs = classid_to_ids[classid]\n",
    "    if len(imgs) == 1:\n",
    "        return []\n",
    "    pos_pairs = [(imgs[i], imgs[j]) for i in range(len(imgs)) for j in range(i+1,len(imgs))]\n",
    "    random.shuffle(pos_pairs)\n",
    "    return pos_pairs[:max_num]\n",
    "\n",
    "def build_neg_pairs_for_id(classid, classes, max_num=20):\n",
    "    imgs = classid_to_ids[classid]\n",
    "    neg_classes_ids = random.sample(classes, max_num+1)\n",
    "    if classid in neg_classes_ids:\n",
    "        neg_classes_ids.remove(classid)\n",
    "    neg_pairs = []\n",
    "    for id2 in range(max_num):\n",
    "        img1 = imgs[random.randint(0,len(imgs)-1)]\n",
    "        imgs2 = classid_to_ids[neg_classes_ids[id2]]\n",
    "        img2 = imgs2[random.randint(0,len(imgs2)-1)]\n",
    "        neg_pairs += [(img1, img2)]\n",
    "    return neg_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build positive and a negative pairs for class 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 7), (7, 8), (5, 8), (5, 6), (6, 8), (6, 7)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_pos_pairs_for_id(5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 6220), (8, 288), (6, 2150), (8, 6515), (7, 3550), (7, 3544)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_neg_pairs_for_id(5, list(range(num_classes)), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to compute the pairs, let's open all the possible images. It will expand all the images into RAM memory. There are more than 1000 images, so 250Mo of RAM will be used, which will not cause any issue.\n",
    "\n",
    "_Note: if you plan on opening more images, you should not open them all at once, and rather build a generator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "def resize100(img):\n",
    "    return resize(img, (100, 100), preserve_range=True, mode='reflect')[20:80,20:80,:]\n",
    "\n",
    "def open_all_images(id_to_path):\n",
    "    all_imgs = []\n",
    "    for path in id_to_path.values():\n",
    "        all_imgs += [np.expand_dims(resize100(imread(PATH+path)),0)]\n",
    "    return np.vstack(all_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs = open_all_images(id_to_path)\n",
    "all_imgs.shape, str(all_imgs.nbytes / 1e6) + \"Mo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test_data(split=0.8):\n",
    "    listX1 = []\n",
    "    listX2 = []\n",
    "    listY = []\n",
    "    split = int(num_classes * split)\n",
    "    \n",
    "    # train\n",
    "    for id in range(split):\n",
    "        pos = build_pos_pairs_for_id(id)\n",
    "        neg = build_neg_pairs_for_id(id, list(range(split)))\n",
    "        for pair in pos:\n",
    "            listX1 += [pair[0]]\n",
    "            listX2 += [pair[1]]\n",
    "            listY += [1]\n",
    "        for pair in neg:\n",
    "            if sum(listY) > len(listY) / 2:\n",
    "                listX1 += [pair[0]]\n",
    "                listX2 += [pair[1]]\n",
    "                listY += [0]\n",
    "    perm = np.random.permutation(listX1)\n",
    "    X1_ids_train, X2_ids_train, Y_ids_train = np.array(listX1)[perm], np.array(listX2)[perm], np.array(listY)[perm]\n",
    "    \n",
    "    listX1 = []\n",
    "    listX2 = []\n",
    "    listY = []\n",
    "    #test\n",
    "    for id in range(split,num_classes):\n",
    "        pos = build_pos_pairs_for_id(id)\n",
    "        neg = build_neg_pairs_for_id(id, list(range(split,num_classes)))\n",
    "        for pair in pos:\n",
    "            listX1 += [pair[0]]\n",
    "            listX2 += [pair[1]]\n",
    "            listY += [1]\n",
    "        for pair in neg:\n",
    "            if sum(listY) > len(listY) / 2:\n",
    "                listX1 += [pair[0]]\n",
    "                listX2 += [pair[1]]\n",
    "                listY += [0]\n",
    "    X1_ids_test, X2_ids_test, Y_ids_test = np.array(listX1), np.array(listX2), np.array(listY)\n",
    "    return X1_ids_train, X2_ids_train, Y_ids_train, X1_ids_test, X2_ids_test, Y_ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    #iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "    iaa.GaussianBlur(sigma=(0, 0.5)), # blur images with a sigma of 0 to 3.0\n",
    "    iaa.Multiply((0.5, 1.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_ids_train, X2_ids_train, train_Y, X1_ids_test, X2_ids_test, test_Y = build_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, X1, X2, Y, batch_size, all_imgs):\n",
    "        self.cur_train_index=0\n",
    "        self.batch_size = batch_size\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "        self.imgs = all_imgs\n",
    "        self.num_samples = Y.shape[0]\n",
    "        \n",
    "    def next_train(self):\n",
    "        while 1:\n",
    "            self.cur_train_index += self.batch_size\n",
    "            if self.cur_train_index >= self.num_samples:\n",
    "                self.cur_train_index=0\n",
    "            \n",
    "            imgs1 = self.X1[self.cur_train_index:self.cur_train_index+self.batch_size]\n",
    "            imgs2 = self.X2[self.cur_train_index:self.cur_train_index+self.batch_size]\n",
    "    \n",
    "       # deactivate augmentation\n",
    "       #     yield ([self.imgs[imgs1], self.imgs[imgs2]],\n",
    "       #             self.Y[self.cur_train_index:self.cur_train_index+self.batch_size])\n",
    "        \n",
    "            yield ([seq.augment_images(self.imgs[imgs1]), \n",
    "                    seq.augment_images(self.imgs[imgs2])\n",
    "                    ],\n",
    "                    self.Y[self.cur_train_index:self.cur_train_index+self.batch_size]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(X1_ids_train, X2_ids_train, train_Y, 32, all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x1, x2], y = next(gen.next_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 6, i + 1)\n",
    "    plt.imshow(x1[i] / 255)\n",
    "    plt.axis('off')\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 6, i + 7)\n",
    "    plt.imshow(x2[i] / 255)\n",
    "    if y[i]==1.0:\n",
    "        plt.title(\"similar\")\n",
    "    else:\n",
    "        plt.title(\"different\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X1 = all_imgs[X1_ids_test]\n",
    "test_X2 = all_imgs[X2_ids_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_X1.shape, test_X2.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 0.25\n",
    "    #return K.mean(K.maximum(y_true * y_pred, 0.), axis=-1)\n",
    "    \n",
    "    return K.mean( y_true * K.square(1 - y_pred) +\n",
    "                  (1 - y_true) * K.square(K.maximum(y_pred - margin, 0)))\n",
    "\n",
    "    #return K.mean( K.square(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_sim(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on similarity.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred > 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((60,60,3), dtype='float32')\n",
    "x = Conv2D(16, 3, activation=\"relu\", padding=\"same\")(inp)\n",
    "x = Conv2D(16, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = MaxPool2D((2,2))(x) # 30,30\n",
    "x = Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = MaxPool2D((2,2))(x) # 15,15\n",
    "x = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = MaxPool2D((2,2))(x) # 8,8\n",
    "x = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(50)(x)\n",
    "shared_conv = Model(inputs=inp, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = Input((60,60,3), dtype='float32')\n",
    "i2 = Input((60,60,3), dtype='float32')\n",
    "\n",
    "x1 = shared_conv(i1)\n",
    "x2 = shared_conv(i2)\n",
    "\n",
    "out = Dot(axes=-1, normalize=True)([x1,x2])\n",
    "\n",
    "model = Model(inputs=[i1, i2], outputs=out)\n",
    "predict_model = Model(inputs=i1, outputs=x1)\n",
    "model.compile(loss=contrastive_loss, optimizer=\"rmsprop\", metrics=[accuracy_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(generator=gen.next_train(), \n",
    "                    steps_per_epoch=train_Y.shape[0] // 32, \n",
    "                    epochs=5,\n",
    "                    validation_data=([test_X1, test_X2], test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=[train_X1, train_X2],\n",
    "          y=train_Y,\n",
    "          epochs=10, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = predict_model.predict(all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_emb = emb / np.linalg.norm(emb, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() > 0.5\n",
    "    return np.mean(pred == y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([test_X1, test_X2])\n",
    "compute_accuracy(test_Y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_sim(x, emb, topn=5):\n",
    "    sims = np.dot(emb,x)\n",
    "    ids = np.argsort(sims)[::-1]\n",
    "    return [(id,sims[id]) for id in ids[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_id(image, emb, topn=5):\n",
    "    sims = np.dot(emb,x)\n",
    "    ids = np.argsort(sims)[::-1]\n",
    "    return [(id,sims[id]) for id in ids[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img):\n",
    "    img = img.astype('uint8')\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_classes = list(filter(lambda x: len(x[1])>4, classid_to_ids.items()))\n",
    "class_idx = random.choice(interesting_classes)[0]\n",
    "print(class_idx)\n",
    "img_idx = random.choice(classid_to_ids[class_idx])\n",
    "for id, sim in most_sim(norm_emb[img_idx], norm_emb):\n",
    "    display(all_imgs[id])\n",
    "    print((classid_to_name[id_to_classid[id]], id, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet loss\n",
    "\n",
    "In the triplet loss model, we'll define 3 inputs $(a,+,-)$ for anchor, positive and negative.\n",
    "\n",
    "#### usage and differences with siamese nets\n",
    "\n",
    "We release the hard constraint that all data of the same class should be squashed to a single point. Rather, they can live on a manifold, as long as they are closer to similar class than "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_positive_pairs(split=0.8):\n",
    "    listX1 = []\n",
    "    listX2 = []\n",
    "    split = int(num_classes * split)\n",
    "    \n",
    "    # train\n",
    "    for id in range(split):\n",
    "        pos = build_pos_pairs_for_id(id)\n",
    "        for pair in pos:\n",
    "            listX1 += [pair[0]]\n",
    "            listX2 += [pair[1]]\n",
    "    perm = np.random.permutation(listX1)\n",
    "    return np.array(listX1)[perm], np.array(listX2)[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa_train, Xp_train = build_positive_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa_train.shape, Xp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletGenerator():\n",
    "    def __init__(self, Xa_train, Xp_train, batch_size, all_imgs, max_positives=20):\n",
    "        self.cur_img_index=0\n",
    "        self.cur_img_pos_index=0\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.imgs = all_imgs\n",
    "        self.Xa = Xa_train\n",
    "        self.Xp = Xp_train\n",
    "        self.cur_train_index = 0\n",
    "        self.num_samples = Xa_train.shape[0]\n",
    "        self.all_imgs_idx = list(range(all_imgs.shape[0]))\n",
    "        \n",
    "    def next_train(self):\n",
    "        while 1:\n",
    "            self.cur_train_index += self.batch_size\n",
    "            if self.cur_train_index >= self.num_samples:\n",
    "                self.cur_train_index=0\n",
    "            \n",
    "            # fill one batch\n",
    "            imgs_a = self.Xa[self.cur_train_index:self.cur_train_index+self.batch_size]\n",
    "            imgs_p = self.Xp[self.cur_train_index:self.cur_train_index+self.batch_size]\n",
    "            imgs_n = random.sample(self.all_imgs_idx,imgs_a.shape[0])\n",
    "       # deactivate augmentation\n",
    "       #     yield ([self.imgs[imgs1], self.imgs[imgs2]],\n",
    "       #             self.Y[self.cur_train_index:self.cur_train_index+self.batch_size])\n",
    "        \n",
    "            yield ([seq.augment_images(self.imgs[imgs_a]), \n",
    "                    seq.augment_images(self.imgs[imgs_p]),\n",
    "                    seq.augment_images(self.imgs[imgs_n])\n",
    "                    ],\n",
    "                    np.zeros(shape=(imgs_a.shape[0]))\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = TripletGenerator(Xa_train, Xp_train, 32, all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[xa, xp, xn], y = next(gen.next_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "for i in range(5):\n",
    "    plt.subplot(3, 5, i + 1)\n",
    "    plt.title(\"anchor\")\n",
    "    plt.imshow(xa[i] / 255)\n",
    "    plt.axis('off')\n",
    "for i in range(5):\n",
    "    plt.subplot(3, 5, i + 6)\n",
    "    plt.title(\"positive\")\n",
    "    plt.imshow(xp[i] / 255)\n",
    "    plt.axis('off')\n",
    "for i in range(5):\n",
    "    plt.subplot(3, 5, i + 11)\n",
    "    plt.title(\"negative\")\n",
    "    plt.imshow(xn[i] / 255)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred - 0 * y_true)\n",
    "\n",
    "\n",
    "def triplet_loss(X):\n",
    "    _alpha = 0.2\n",
    "    a, p, n = X\n",
    "\n",
    "    positive_distances = K.mean(K.square(a - p),axis=-1)\n",
    "    negative_distances = K.mean(K.square(a - n),axis=-1)\n",
    "    \n",
    "    # batch loss\n",
    "    losses = K.maximum(0.0, positive_distances - negative_distances + _alpha)\n",
    "    \n",
    "    return K.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalizeLayer = Lambda(lambda x: K.l2_normalize(\n",
    "                        K.sqrt(K.relu(x) + K.epsilon()) - K.sqrt(K.relu(-x)+ K.epsilon()),\n",
    "                        axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((60,60,3), dtype='float32')\n",
    "x = Conv2D(16, 3, activation=\"relu\", padding=\"same\")(inp)\n",
    "x = Conv2D(16, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = MaxPool2D((2,2))(x) # 30,30\n",
    "x = Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = MaxPool2D((2,2))(x) # 15,15\n",
    "x = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = MaxPool2D((2,2))(x) # 8,8\n",
    "x = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(50)(x)\n",
    "shared_conv2 = Model(inputs=inp, outputs = x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tech details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = Input((60, 60, 3), name='anchor')\n",
    "positive = Input((60, 60, 3), name='positive')\n",
    "negative = Input((60, 60, 3), name='negative')\n",
    "\n",
    "a = shared_conv2(anchor)\n",
    "p = shared_conv2(positive)\n",
    "n = shared_conv2(negative)\n",
    "\n",
    "loss = Lambda(triplet_loss,\n",
    "                      output_shape=(1,))(\n",
    "                      [a,p,n])\n",
    "\n",
    "model_triplet = Model(\n",
    "    inputs=[anchor, positive, negative],\n",
    "    outputs=loss)\n",
    "\n",
    "predict_model_triplet = Model(inputs=anchor, outputs=a)\n",
    "model_triplet.compile(loss=identity_loss, optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_triplet.fit_generator(generator=gen.next_train(), \n",
    "                    steps_per_epoch=Xa_train.shape[0] // 32, \n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = predict_model_triplet.predict(all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_emb = emb / np.linalg.norm(emb, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_sim(x, emb, topn=5):\n",
    "    sims = np.dot(emb,x)\n",
    "    ids = np.argsort(sims)[::-1]\n",
    "    return [(id,sims[id]) for id in ids[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_id(image, emb, topn=5):\n",
    "    sims = np.dot(emb,x)\n",
    "    ids = np.argsort(sims)[::-1]\n",
    "    return [(id,sims[id]) for id in ids[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img):\n",
    "    img = img.astype('uint8')\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_classes = list(filter(lambda x: len(x[1])>4, classid_to_ids.items()))\n",
    "class_idx = random.choice(interesting_classes)[0]\n",
    "print(class_idx)\n",
    "img_idx = random.choice(classid_to_ids[class_idx])\n",
    "for id, sim in most_sim(norm_emb[img_idx], norm_emb):\n",
    "    display(all_imgs[id])\n",
    "    print((classid_to_name[id_to_classid[id]], id, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo \n",
    "- compute ROC AuC\n",
    "- Hard negative mining\n",
    "- pretrained model\n",
    "- exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
