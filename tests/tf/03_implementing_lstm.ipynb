{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an LSTM RNN Model\n",
    "------------------------\n",
    "Here we implement an LSTM model on all a data set of Shakespeare works.\n",
    "\n",
    "We start by loading the necessary libraries and resetting the default computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start a computational graph session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is important to set the algorithm and data processing parameters.\n",
    "\n",
    "---------\n",
    "Parameter  :  Descriptions\n",
    " - min_word_freq: Only attempt to model words that appear at least 5 times.\n",
    " - rnn_size: size of our RNN (equal to the embedding size)\n",
    " - epochs: Number of epochs to cycle through the data\n",
    " - batch_size: How many examples to train on at once\n",
    " - learning_rate: The learning rate or the convergence paramter\n",
    " - training_seq_len: The length of the surrounding word group (e.g. 10 = 5 on each side)\n",
    " - embedding_size: Must be equal to the rnn_size\n",
    " - save_every: How often to save the model\n",
    " - eval_every: How often to evaluate the model\n",
    " - prime_texts: List of test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RNN Parameters\n",
    "min_word_freq = 5 # Trim the less frequent words off\n",
    "rnn_size = 128 # RNN Model size\n",
    "embedding_size = 100 # Word embedding size\n",
    "epochs = 10 # Number of epochs to cycle through data\n",
    "batch_size = 100 # Train on this many examples at once\n",
    "learning_rate = 0.001 # Learning rate\n",
    "training_seq_len = 50 # how long of a word group to consider \n",
    "embedding_size = rnn_size\n",
    "save_every = 500 # How often to save model checkpoints\n",
    "eval_every = 50 # How often to evaluate the test sentences\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']\n",
    "\n",
    "# Download/store Shakespeare data\n",
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "\n",
    "# Declare punctuation to remove, everything except hyphens and apostrophes\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "\n",
    "# Make data directory\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data if we don't have it saved already.  The data comes from the [Gutenberg Project](http://www.gutenberg.org])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare Data\n",
      "Not found, downloading Shakespeare texts from www.gutenberg.org\n",
      "Cleaning Text\n",
      "Done loading/cleaning.\n"
     ]
    }
   ],
   "source": [
    "print('Loading Shakespeare Data')\n",
    "# Check if file is downloaded.\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    # Get Shakespeare text\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    # Decode binary into string\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    # Drop first few descriptive paragraphs.\n",
    "    s_text = s_text[7675:]\n",
    "    # Remove newlines\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    \n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    # If file has been saved, load from that file\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')\n",
    "\n",
    "# Clean text\n",
    "print('Cleaning Text')\n",
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text ).strip().lower()\n",
    "print('Done loading/cleaning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to build a word processing dictionary (word -> ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word vocabulary function\n",
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    # limit word counts to those more frequent than cutoff\n",
    "    word_counts = {key:val for key, val in word_counts.items() if val>min_word_freq}\n",
    "    # Create vocab --> index mapping\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    # Add unknown key --> 0 index\n",
    "    vocab_to_ix_dict['unknown']=0\n",
    "    # Create index --> vocab mapping\n",
    "    ix_to_vocab_dict = {val:key for key,val in vocab_to_ix_dict.items()}\n",
    "    \n",
    "    return(ix_to_vocab_dict, vocab_to_ix_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the index-vocabulary from the Shakespeare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Shakespeare Vocab\n",
      "Vocabulary Length = 8009\n"
     ]
    }
   ],
   "source": [
    "# Build Shakespeare vocabulary\n",
    "print('Building Shakespeare Vocab')\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1\n",
    "print('Vocabulary Length = {}'.format(vocab_size))\n",
    "# Sanity Check\n",
    "assert(len(ix2vocab) == len(vocab2ix))\n",
    "\n",
    "# Convert text to word vectors\n",
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the LSTM model.  The methods of interest are the `__init__()` method, which defines all the model variables and operations, and the `sample()` method which takes in a sample word and loops through to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM RNN Model\n",
    "class LSTM_Model():\n",
    "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.embedding_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # If we are inferring (generating text), we add a 'loop' function\n",
    "        # Define how to get the i+1 th input from the i th output\n",
    "        def inferred_loop(prev, count):\n",
    "            # Apply hidden layer\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # Get the index of the output (also don't run the gradient)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # Get embedded vector\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return(output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return(out_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the same model (with the same trained variables), we need to share the variable scope between the trained model and the test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM Model\n",
    "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                        training_seq_len, vocab_size)\n",
    "\n",
    "# Tell TensorFlow we are reusing the scope for the testing\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                                 training_seq_len, vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to save the model, so we create a model saving operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model saver\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate how many batches are needed for each epoch and split up the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches for each epoch\n",
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n",
    "# Split up text indices into subarrays, of equal size\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "# Reshape each split into [batch_size, training_seq_len]\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch #1 of 10.\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 9.89\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182, Loss: 8.97\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182, Loss: 8.57\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182, Loss: 8.33\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182, Loss: 7.97\n",
      "thou art more syracusian i have\n",
      "to be or not to the\n",
      "wherefore art thou hither '- arise\n",
      "Iteration: 60, Epoch: 1, Batch: 60 out of 182, Loss: 7.63\n",
      "Iteration: 70, Epoch: 1, Batch: 70 out of 182, Loss: 7.63\n",
      "Iteration: 80, Epoch: 1, Batch: 80 out of 182, Loss: 7.50\n",
      "Iteration: 90, Epoch: 1, Batch: 90 out of 182, Loss: 7.13\n",
      "Iteration: 100, Epoch: 1, Batch: 100 out of 182, Loss: 7.13\n",
      "thou art more in the\n",
      "to be or not to the\n",
      "wherefore art thou not be\n",
      "Iteration: 110, Epoch: 1, Batch: 110 out of 182, Loss: 7.01\n",
      "Iteration: 120, Epoch: 1, Batch: 120 out of 182, Loss: 6.91\n",
      "Iteration: 130, Epoch: 1, Batch: 130 out of 182, Loss: 6.96\n",
      "Iteration: 140, Epoch: 1, Batch: 140 out of 182, Loss: 6.58\n",
      "Iteration: 150, Epoch: 1, Batch: 150 out of 182, Loss: 6.71\n",
      "thou art more in the\n",
      "to be or not to the\n",
      "wherefore art thou not\n",
      "Iteration: 160, Epoch: 1, Batch: 160 out of 182, Loss: 6.66\n",
      "Iteration: 170, Epoch: 1, Batch: 170 out of 182, Loss: 6.55\n",
      "Iteration: 180, Epoch: 1, Batch: 180 out of 182, Loss: 6.55\n",
      "Starting Epoch #2 of 10.\n",
      "Iteration: 190, Epoch: 2, Batch: 9 out of 182, Loss: 6.59\n",
      "Iteration: 200, Epoch: 2, Batch: 19 out of 182, Loss: 6.58\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou not be\n",
      "Iteration: 210, Epoch: 2, Batch: 29 out of 182, Loss: 6.46\n",
      "Iteration: 220, Epoch: 2, Batch: 39 out of 182, Loss: 6.40\n",
      "Iteration: 230, Epoch: 2, Batch: 49 out of 182, Loss: 6.45\n",
      "Iteration: 240, Epoch: 2, Batch: 59 out of 182, Loss: 6.47\n",
      "Iteration: 250, Epoch: 2, Batch: 69 out of 182, Loss: 6.24\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou not be\n",
      "Iteration: 260, Epoch: 2, Batch: 79 out of 182, Loss: 6.36\n",
      "Iteration: 270, Epoch: 2, Batch: 89 out of 182, Loss: 6.34\n",
      "Iteration: 280, Epoch: 2, Batch: 99 out of 182, Loss: 6.37\n",
      "Iteration: 290, Epoch: 2, Batch: 109 out of 182, Loss: 6.11\n",
      "Iteration: 300, Epoch: 2, Batch: 119 out of 182, Loss: 6.42\n",
      "thou art more than i am not to the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou not be\n",
      "Iteration: 310, Epoch: 2, Batch: 129 out of 182, Loss: 6.54\n",
      "Iteration: 320, Epoch: 2, Batch: 139 out of 182, Loss: 6.68\n",
      "Iteration: 330, Epoch: 2, Batch: 149 out of 182, Loss: 6.61\n",
      "Iteration: 340, Epoch: 2, Batch: 159 out of 182, Loss: 6.58\n",
      "Iteration: 350, Epoch: 2, Batch: 169 out of 182, Loss: 6.51\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 360, Epoch: 2, Batch: 179 out of 182, Loss: 6.22\n",
      "Starting Epoch #3 of 10.\n",
      "Iteration: 370, Epoch: 3, Batch: 8 out of 182, Loss: 6.01\n",
      "Iteration: 380, Epoch: 3, Batch: 18 out of 182, Loss: 6.23\n",
      "Iteration: 390, Epoch: 3, Batch: 28 out of 182, Loss: 6.41\n",
      "Iteration: 400, Epoch: 3, Batch: 38 out of 182, Loss: 6.35\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 410, Epoch: 3, Batch: 48 out of 182, Loss: 6.01\n",
      "Iteration: 420, Epoch: 3, Batch: 58 out of 182, Loss: 6.41\n",
      "Iteration: 430, Epoch: 3, Batch: 68 out of 182, Loss: 6.24\n",
      "Iteration: 440, Epoch: 3, Batch: 78 out of 182, Loss: 6.33\n",
      "Iteration: 450, Epoch: 3, Batch: 88 out of 182, Loss: 6.23\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 460, Epoch: 3, Batch: 98 out of 182, Loss: 6.18\n",
      "Iteration: 470, Epoch: 3, Batch: 108 out of 182, Loss: 6.15\n",
      "Iteration: 480, Epoch: 3, Batch: 118 out of 182, Loss: 6.23\n",
      "Iteration: 490, Epoch: 3, Batch: 128 out of 182, Loss: 6.19\n",
      "Iteration: 500, Epoch: 3, Batch: 138 out of 182, Loss: 6.31\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou hast thou hast thou hast thou\n",
      "Iteration: 510, Epoch: 3, Batch: 148 out of 182, Loss: 6.02\n",
      "Iteration: 520, Epoch: 3, Batch: 158 out of 182, Loss: 6.06\n",
      "Iteration: 530, Epoch: 3, Batch: 168 out of 182, Loss: 6.02\n",
      "Iteration: 540, Epoch: 3, Batch: 178 out of 182, Loss: 5.91\n",
      "Starting Epoch #4 of 10.\n",
      "Iteration: 550, Epoch: 4, Batch: 7 out of 182, Loss: 6.22\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou hast thou hast thou art thou hast thou\n",
      "Iteration: 560, Epoch: 4, Batch: 17 out of 182, Loss: 6.06\n",
      "Iteration: 570, Epoch: 4, Batch: 27 out of 182, Loss: 5.79\n",
      "Iteration: 580, Epoch: 4, Batch: 37 out of 182, Loss: 6.13\n",
      "Iteration: 590, Epoch: 4, Batch: 47 out of 182, Loss: 6.19\n",
      "Iteration: 600, Epoch: 4, Batch: 57 out of 182, Loss: 6.11\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 610, Epoch: 4, Batch: 67 out of 182, Loss: 6.19\n",
      "Iteration: 620, Epoch: 4, Batch: 77 out of 182, Loss: 6.11\n",
      "Iteration: 630, Epoch: 4, Batch: 87 out of 182, Loss: 6.30\n",
      "Iteration: 640, Epoch: 4, Batch: 97 out of 182, Loss: 6.11\n",
      "Iteration: 650, Epoch: 4, Batch: 107 out of 182, Loss: 6.12\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou hast thou hast thou art thou art thou\n",
      "Iteration: 660, Epoch: 4, Batch: 117 out of 182, Loss: 6.06\n",
      "Iteration: 670, Epoch: 4, Batch: 127 out of 182, Loss: 6.08\n",
      "Iteration: 680, Epoch: 4, Batch: 137 out of 182, Loss: 5.90\n",
      "Iteration: 690, Epoch: 4, Batch: 147 out of 182, Loss: 6.29\n",
      "Iteration: 700, Epoch: 4, Batch: 157 out of 182, Loss: 5.86\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 710, Epoch: 4, Batch: 167 out of 182, Loss: 6.08\n",
      "Iteration: 720, Epoch: 4, Batch: 177 out of 182, Loss: 6.17\n",
      "Starting Epoch #5 of 10.\n",
      "Iteration: 730, Epoch: 5, Batch: 6 out of 182, Loss: 6.10\n",
      "Iteration: 740, Epoch: 5, Batch: 16 out of 182, Loss: 6.24\n",
      "Iteration: 750, Epoch: 5, Batch: 26 out of 182, Loss: 6.18\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 760, Epoch: 5, Batch: 36 out of 182, Loss: 5.93\n",
      "Iteration: 770, Epoch: 5, Batch: 46 out of 182, Loss: 6.04\n",
      "Iteration: 780, Epoch: 5, Batch: 56 out of 182, Loss: 6.05\n",
      "Iteration: 790, Epoch: 5, Batch: 66 out of 182, Loss: 5.88\n",
      "Iteration: 800, Epoch: 5, Batch: 76 out of 182, Loss: 5.97\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou art thou art thou\n",
      "Iteration: 810, Epoch: 5, Batch: 86 out of 182, Loss: 5.92\n",
      "Iteration: 820, Epoch: 5, Batch: 96 out of 182, Loss: 6.00\n",
      "Iteration: 830, Epoch: 5, Batch: 106 out of 182, Loss: 6.27\n",
      "Iteration: 840, Epoch: 5, Batch: 116 out of 182, Loss: 6.08\n",
      "Iteration: 850, Epoch: 5, Batch: 126 out of 182, Loss: 5.97\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art thou hast thou art thou art thou art thou\n",
      "Iteration: 860, Epoch: 5, Batch: 136 out of 182, Loss: 6.25\n",
      "Iteration: 870, Epoch: 5, Batch: 146 out of 182, Loss: 5.81\n",
      "Iteration: 880, Epoch: 5, Batch: 156 out of 182, Loss: 6.19\n",
      "Iteration: 890, Epoch: 5, Batch: 166 out of 182, Loss: 5.66\n",
      "Iteration: 900, Epoch: 5, Batch: 176 out of 182, Loss: 6.01\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Starting Epoch #6 of 10.\n",
      "Iteration: 910, Epoch: 6, Batch: 5 out of 182, Loss: 6.08\n",
      "Iteration: 920, Epoch: 6, Batch: 15 out of 182, Loss: 6.00\n",
      "Iteration: 930, Epoch: 6, Batch: 25 out of 182, Loss: 5.90\n",
      "Iteration: 940, Epoch: 6, Batch: 35 out of 182, Loss: 5.90\n",
      "Iteration: 950, Epoch: 6, Batch: 45 out of 182, Loss: 5.92\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast not so\n",
      "Iteration: 960, Epoch: 6, Batch: 55 out of 182, Loss: 6.10\n",
      "Iteration: 970, Epoch: 6, Batch: 65 out of 182, Loss: 5.82\n",
      "Iteration: 980, Epoch: 6, Batch: 75 out of 182, Loss: 5.97\n",
      "Iteration: 990, Epoch: 6, Batch: 85 out of 182, Loss: 6.04\n",
      "Iteration: 1000, Epoch: 6, Batch: 95 out of 182, Loss: 5.89\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou art not a\n",
      "Iteration: 1010, Epoch: 6, Batch: 105 out of 182, Loss: 5.77\n",
      "Iteration: 1020, Epoch: 6, Batch: 115 out of 182, Loss: 6.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1030, Epoch: 6, Batch: 125 out of 182, Loss: 5.91\n",
      "Iteration: 1040, Epoch: 6, Batch: 135 out of 182, Loss: 5.93\n",
      "Iteration: 1050, Epoch: 6, Batch: 145 out of 182, Loss: 6.01\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 1060, Epoch: 6, Batch: 155 out of 182, Loss: 5.93\n",
      "Iteration: 1070, Epoch: 6, Batch: 165 out of 182, Loss: 5.90\n",
      "Iteration: 1080, Epoch: 6, Batch: 175 out of 182, Loss: 5.87\n",
      "Starting Epoch #7 of 10.\n",
      "Iteration: 1090, Epoch: 7, Batch: 4 out of 182, Loss: 5.89\n",
      "Iteration: 1100, Epoch: 7, Batch: 14 out of 182, Loss: 5.82\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 1110, Epoch: 7, Batch: 24 out of 182, Loss: 5.85\n",
      "Iteration: 1120, Epoch: 7, Batch: 34 out of 182, Loss: 5.85\n",
      "Iteration: 1130, Epoch: 7, Batch: 44 out of 182, Loss: 5.93\n",
      "Iteration: 1140, Epoch: 7, Batch: 54 out of 182, Loss: 5.86\n",
      "Iteration: 1150, Epoch: 7, Batch: 64 out of 182, Loss: 5.84\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 1160, Epoch: 7, Batch: 74 out of 182, Loss: 5.90\n",
      "Iteration: 1170, Epoch: 7, Batch: 84 out of 182, Loss: 6.04\n",
      "Iteration: 1180, Epoch: 7, Batch: 94 out of 182, Loss: 5.83\n",
      "Iteration: 1190, Epoch: 7, Batch: 104 out of 182, Loss: 5.96\n",
      "Iteration: 1200, Epoch: 7, Batch: 114 out of 182, Loss: 5.98\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou art not so\n",
      "Iteration: 1210, Epoch: 7, Batch: 124 out of 182, Loss: 5.98\n",
      "Iteration: 1220, Epoch: 7, Batch: 134 out of 182, Loss: 6.18\n",
      "Iteration: 1230, Epoch: 7, Batch: 144 out of 182, Loss: 5.86\n",
      "Iteration: 1240, Epoch: 7, Batch: 154 out of 182, Loss: 5.77\n",
      "Iteration: 1250, Epoch: 7, Batch: 164 out of 182, Loss: 5.59\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 1260, Epoch: 7, Batch: 174 out of 182, Loss: 5.89\n",
      "Starting Epoch #8 of 10.\n",
      "Iteration: 1270, Epoch: 8, Batch: 3 out of 182, Loss: 5.78\n",
      "Iteration: 1280, Epoch: 8, Batch: 13 out of 182, Loss: 5.95\n",
      "Iteration: 1290, Epoch: 8, Batch: 23 out of 182, Loss: 5.78\n",
      "Iteration: 1300, Epoch: 8, Batch: 33 out of 182, Loss: 5.91\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou art a\n",
      "Iteration: 1310, Epoch: 8, Batch: 43 out of 182, Loss: 5.78\n",
      "Iteration: 1320, Epoch: 8, Batch: 53 out of 182, Loss: 5.68\n",
      "Iteration: 1330, Epoch: 8, Batch: 63 out of 182, Loss: 5.78\n",
      "Iteration: 1340, Epoch: 8, Batch: 73 out of 182, Loss: 6.01\n",
      "Iteration: 1350, Epoch: 8, Batch: 83 out of 182, Loss: 5.67\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast not so much as i have\n",
      "Iteration: 1360, Epoch: 8, Batch: 93 out of 182, Loss: 5.87\n",
      "Iteration: 1370, Epoch: 8, Batch: 103 out of 182, Loss: 5.91\n",
      "Iteration: 1380, Epoch: 8, Batch: 113 out of 182, Loss: 5.76\n",
      "Iteration: 1390, Epoch: 8, Batch: 123 out of 182, Loss: 5.99\n",
      "Iteration: 1400, Epoch: 8, Batch: 133 out of 182, Loss: 5.61\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast not so much as i have\n",
      "Iteration: 1410, Epoch: 8, Batch: 143 out of 182, Loss: 5.97\n",
      "Iteration: 1420, Epoch: 8, Batch: 153 out of 182, Loss: 5.74\n",
      "Iteration: 1430, Epoch: 8, Batch: 163 out of 182, Loss: 5.63\n",
      "Iteration: 1440, Epoch: 8, Batch: 173 out of 182, Loss: 5.98\n",
      "Starting Epoch #9 of 10.\n",
      "Iteration: 1450, Epoch: 9, Batch: 2 out of 182, Loss: 5.52\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast not so much as i have\n",
      "Iteration: 1460, Epoch: 9, Batch: 12 out of 182, Loss: 5.70\n",
      "Iteration: 1470, Epoch: 9, Batch: 22 out of 182, Loss: 5.81\n",
      "Iteration: 1480, Epoch: 9, Batch: 32 out of 182, Loss: 5.63\n",
      "Iteration: 1490, Epoch: 9, Batch: 42 out of 182, Loss: 5.82\n",
      "Iteration: 1500, Epoch: 9, Batch: 52 out of 182, Loss: 5.79\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast not so much as i am\n",
      "Iteration: 1510, Epoch: 9, Batch: 62 out of 182, Loss: 5.82\n",
      "Iteration: 1520, Epoch: 9, Batch: 72 out of 182, Loss: 5.98\n",
      "Iteration: 1530, Epoch: 9, Batch: 82 out of 182, Loss: 5.80\n",
      "Iteration: 1540, Epoch: 9, Batch: 92 out of 182, Loss: 5.60\n",
      "Iteration: 1550, Epoch: 9, Batch: 102 out of 182, Loss: 5.75\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast not so much as i am a\n",
      "Iteration: 1560, Epoch: 9, Batch: 112 out of 182, Loss: 6.05\n",
      "Iteration: 1570, Epoch: 9, Batch: 122 out of 182, Loss: 5.80\n",
      "Iteration: 1580, Epoch: 9, Batch: 132 out of 182, Loss: 5.44\n",
      "Iteration: 1590, Epoch: 9, Batch: 142 out of 182, Loss: 5.69\n",
      "Iteration: 1600, Epoch: 9, Batch: 152 out of 182, Loss: 5.87\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 1610, Epoch: 9, Batch: 162 out of 182, Loss: 5.81\n",
      "Iteration: 1620, Epoch: 9, Batch: 172 out of 182, Loss: 5.72\n",
      "Starting Epoch #10 of 10.\n",
      "Iteration: 1630, Epoch: 10, Batch: 1 out of 182, Loss: 5.86\n",
      "Iteration: 1640, Epoch: 10, Batch: 11 out of 182, Loss: 5.98\n",
      "Iteration: 1650, Epoch: 10, Batch: 21 out of 182, Loss: 5.77\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast not so much as i have done to the\n",
      "Iteration: 1660, Epoch: 10, Batch: 31 out of 182, Loss: 5.80\n",
      "Iteration: 1670, Epoch: 10, Batch: 41 out of 182, Loss: 5.78\n",
      "Iteration: 1680, Epoch: 10, Batch: 51 out of 182, Loss: 5.85\n",
      "Iteration: 1690, Epoch: 10, Batch: 61 out of 182, Loss: 5.81\n",
      "Iteration: 1700, Epoch: 10, Batch: 71 out of 182, Loss: 5.64\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast not so much as i have done the\n",
      "Iteration: 1710, Epoch: 10, Batch: 81 out of 182, Loss: 5.95\n",
      "Iteration: 1720, Epoch: 10, Batch: 91 out of 182, Loss: 5.70\n",
      "Iteration: 1730, Epoch: 10, Batch: 101 out of 182, Loss: 5.70\n",
      "Iteration: 1740, Epoch: 10, Batch: 111 out of 182, Loss: 5.84\n",
      "Iteration: 1750, Epoch: 10, Batch: 121 out of 182, Loss: 5.79\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast not so much as i have\n",
      "Iteration: 1760, Epoch: 10, Batch: 131 out of 182, Loss: 5.69\n",
      "Iteration: 1770, Epoch: 10, Batch: 141 out of 182, Loss: 5.75\n",
      "Iteration: 1780, Epoch: 10, Batch: 151 out of 182, Loss: 5.75\n",
      "Iteration: 1790, Epoch: 10, Batch: 161 out of 182, Loss: 5.79\n",
      "Iteration: 1800, Epoch: 10, Batch: 171 out of 182, Loss: 6.01\n",
      "thou art more than my lord i am not to the\n",
      "to be or not to the\n",
      "wherefore art thou hast not so much as i am not to the\n",
      "Iteration: 1810, Epoch: 10, Batch: 181 out of 182, Loss: 5.77\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle word indices\n",
    "    random.shuffle(batches)\n",
    "    # Create targets from shuffled batches\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    # Run a through one epoch\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch+1, epochs))\n",
    "    # Reset initial LSTM state every epoch\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        # Print status every 10 gens\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "        # Save the model and the vocab\n",
    "        if iteration_count % save_every == 0:\n",
    "            # Save model\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step = iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            # Save vocabulary\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "                \n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a plot of the training loss across the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f178c40c470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss over time\n",
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test complete; Gopal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
